{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-7e7fe1892768>:2: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data.\n",
      "WARNING:tensorflow:From /home/nlp/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:78: load_dbpedia (from tensorflow.contrib.learn.python.learn.datasets.text_datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "See contrib/learn/README.md\n",
      "WARNING:tensorflow:From /home/nlp/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py:56: maybe_download_dbpedia (from tensorflow.contrib.learn.python.learn.datasets.text_datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "See contrib/learn/README.md\n",
      "WARNING:tensorflow:From /home/nlp/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py:46: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/nlp/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded dbpedia_csv.tar.gz 68431223 bytes.\n",
      "WARNING:tensorflow:From /home/nlp/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py:63: shrink_csv (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.data instead.\n",
      "WARNING:tensorflow:From /home/nlp/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py:73: load_csv_without_header (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.data instead.\n"
     ]
    }
   ],
   "source": [
    "dbpedia = tf.contrib.learn.datasets.load_dataset(\n",
    "      'dbpedia', test_with_fake_data=False, size='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame(dbpedia.train.data)[1]\n",
    "y_train = pd.Series(dbpedia.train.target)\n",
    "x_test = pd.DataFrame(dbpedia.test.data)[1]\n",
    "y_test = pd.Series(dbpedia.test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.\n",
      "248\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(len(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process vocabulary\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "char_processor = tf.contrib.learn.preprocessing.ByteProcessor(\n",
    "  MAX_DOCUMENT_LENGTH)\n",
    "x_train = np.array(list(char_processor.fit_transform(x_train)))\n",
    "x_test = np.array(list(char_processor.transform(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 32,  65,  98,  98, 111, 116, 116,  32, 111, 102,  32,  70,  97,\n",
       "       114, 110, 104,  97, 109,  32,  69,  32,  68,  32,  65,  98,  98,\n",
       "       111, 116, 116,  32,  76, 105, 109, 105, 116, 101, 100,  32, 119,\n",
       "        97, 115,  32,  97,  32,  66, 114, 105, 116, 105, 115, 104,  32,\n",
       "        99, 111,  97,  99, 104,  98, 117, 105, 108, 100, 105, 110, 103,\n",
       "        32,  98, 117, 115, 105, 110, 101, 115, 115,  32,  98,  97, 115,\n",
       "       101, 100,  32, 105, 110,  32,  70,  97, 114, 110, 104,  97, 109,\n",
       "        32,  83, 117, 114, 114, 101, 121,  32, 116], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = 'data/vocab-2016-09-10.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from char_data import Batcher\n",
    "# Create a Batcher to map text to character ids.\n",
    "vocab_file = 'data/vocab-2016-09-10.txt'\n",
    "batcher = Batcher(vocab_file, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input placeholders to the biLM.\n",
    "context_character_ids = tf.placeholder('int32', shape=(None, None, 50))\n",
    "question_character_ids = tf.placeholder('int32', shape=(None, None, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can compute embeddings.\n",
    "raw_context = [\n",
    "    'Pretrained biLMs compute representations useful for NLP tasks .',\n",
    "    'They give state of the art performance for many tasks .'\n",
    "]\n",
    "tokenized_context = [sentence.split() for sentence in raw_context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_ids = batcher.batch_sentences(tokenized_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 13, 50)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_size = 2000\n",
    "    embedding_size = 100\n",
    "    filter_size = 64\n",
    "    num_layers = 3\n",
    "    block_size = 3\n",
    "    filter_h = 5\n",
    "    context_size = 20\n",
    "    text_size = context_size\n",
    "    batch_size = 32\n",
    "    epochs = 5\n",
    "    num_sampled = 64\n",
    "    learning_rate = 0.0001\n",
    "    momentum = 0.99\n",
    "    grad_clip = 0.1\n",
    "    num_batches = 0\n",
    "    ckpt_path = 'ckpt'\n",
    "    summary_path = 'logs'\n",
    "    #data_dir = \"data/texts/reviews/movie_reviews\"\n",
    "    data_dir = \"data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from model import GatedCharCNN\n",
    "from conf_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please download the data as mentioned in Requirements\n",
      "Tensor(\"concat:0\", shape=(?, ?, 124), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(?, ?, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Initialize configuration files\n",
    "conf = prepare_conf(config)\n",
    "gcnn = GatedCharCNN(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from bilm.training import train, load_options_latest_checkpoint, load_vocab\n",
    "from bilm.data import BidirectionalLMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = 'data/vocab-2016-09-10.txt'\n",
    "vocab = load_vocab(vocab_file, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the options\n",
    "batch_size = 128  # batch size for each GPU\n",
    "n_gpus = 3\n",
    "\n",
    "# number of tokens in training data (this for 1B Word Benchmark)\n",
    "n_train_tokens = 768648884\n",
    "\n",
    "options = {\n",
    "     'bidirectional': True,\n",
    "\n",
    "     'char_cnn': {'activation': 'relu',\n",
    "      'embedding': {'dim': 16},\n",
    "      'filters': [[1, 32],\n",
    "       [2, 32],\n",
    "       [3, 64],\n",
    "       [4, 128],\n",
    "       [5, 256],\n",
    "       [6, 512],\n",
    "       [7, 1024]],\n",
    "      'max_characters_per_token': 50,\n",
    "      'n_characters': 261,\n",
    "      'n_highway': 2},\n",
    "    \n",
    "     'dropout': 0.1,\n",
    "    \n",
    "     'lstm': {\n",
    "      'cell_clip': 3,\n",
    "      'dim': 4096,\n",
    "      'n_layers': 2,\n",
    "      'proj_clip': 3,\n",
    "      'projection_dim': 512,\n",
    "      'use_skip_connections': True},\n",
    "    \n",
    "     'all_clip_norm_val': 10.0,\n",
    "    \n",
    "     'n_epochs': 10,\n",
    "     'n_train_tokens': n_train_tokens,\n",
    "     'batch_size': batch_size,\n",
    "     'n_tokens_vocab': vocab.size,\n",
    "     'unroll_steps': 20,\n",
    "     'n_negative_samples_batch': 8192,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prefix = 'data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 99 shards at data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00034-of-00100\n",
      "Loaded 305408 sentences.\n",
      "Finished loading\n",
      "Found 99 shards at data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00080-of-00100\n",
      "Loaded 305615 sentences.\n",
      "Finished loading\n"
     ]
    }
   ],
   "source": [
    "data = BidirectionalLMDataset(train_prefix, vocab, test=False,\n",
    "                                      shuffle_on_load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = data.iter_batches(batch_size * 1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00027-of-00100\n",
      "Loaded 306804 sentences.\n",
      "Finished loading\n"
     ]
    }
   ],
   "source": [
    "for batch_no, batch in enumerate(data_gen, start=1):\n",
    "\n",
    "            # slice the input in the batch for the feed_dict\n",
    "            X = batch\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 20, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['tokens_characters'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token_ids', 'tokens_characters', 'next_token_id', 'token_ids_reverse', 'tokens_characters_reverse', 'next_token_id_reverse'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
