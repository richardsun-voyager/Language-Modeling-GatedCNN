{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep contextualized word representation has drawn wide attention because of state-of-the-art performances in downstream tasks. Contextualized embeddings can capture not only word-level information but also multi-sense information, thus improving the results in sentiment analysis, SQuad and etc. However, the language adopted in the [Elmo](https://allennlp.org/elmo) model were biLSTMs which contained a huge number of parameters, it was less likely for small labs to train and run such experiments.\n",
    "\n",
    "\n",
    "In this project, we intend to make use of CNN language model in learning efficient word representations for sentiment analysis. We train a language model based on [Gated CNN architecture](https://arxiv.org/abs/1612.08083) proposed by Yann Daulphin, then do sentiment analysis with embeddings generated by the language model.\n",
    "\n",
    "The language model training dataset is 1-billion-word-language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "\n",
    "from model import *\n",
    "from data_utils import data_helper\n",
    "from conf_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the configuration and prepare data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_size = 2000\n",
    "    embedding_size = 200\n",
    "    filter_size = 64\n",
    "    num_layers = 3\n",
    "    block_size = 3\n",
    "    filter_h = 5\n",
    "    context_size = 20\n",
    "    text_size = context_size\n",
    "    batch_size = 32\n",
    "    epochs = 5\n",
    "    num_sampled = 64\n",
    "    learning_rate = 0.0001\n",
    "    momentum = 0.99\n",
    "    grad_clip = 0.1\n",
    "    num_batches = 0\n",
    "    ckpt_path = 'ckpt'\n",
    "    summary_path = 'logs'\n",
    "    #data_dir = \"data/texts/reviews/movie_reviews\"\n",
    "    data_dir = \"data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please download the data as mentioned in Requirements\n"
     ]
    }
   ],
   "source": [
    "#Initialize configuration files\n",
    "conf = prepare_conf(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data batches for language model training\n",
    "dh = data_helper(conf)\n",
    "x_batches, y_batches, word_to_idx, idx_to_word = dh.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.text_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_batches[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a CNN-based language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.text_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(640, 200), dtype=float32)\n",
      "Started Model Training...\n"
     ]
    }
   ],
   "source": [
    "#Create a language model\n",
    "#Note we need to save the models for subsequent tasks\n",
    "model = GatedCNN(conf)\n",
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "print(\"Started Model Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from RNN import RNN\n",
    "#model = RNN(conf)\n",
    "#saver = tf.train.Saver(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32895"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223.95566\n",
      "209.2966\n",
      "152.3483\n",
      "101.426\n",
      "68.43827\n",
      "77.2864\n",
      "66.055534\n",
      "59.393745\n",
      "38.960575\n",
      "45.392616\n",
      "17.473696\n",
      "29.895039\n",
      "21.156122\n",
      "27.863947\n",
      "32.53392\n",
      "23.234015\n",
      "26.559147\n",
      "9.5749\n",
      "6.820913\n",
      "9.40538\n",
      "7.2287917\n",
      "6.42615\n",
      "4.7389736\n",
      "5.236864\n",
      "4.5747757\n",
      "4.6145086\n",
      "5.0947075\n",
      "4.5440392\n",
      "4.839633\n",
      "5.118791\n",
      "5.0830765\n",
      "4.748791\n",
      "4.250371\n",
      "4.3809414\n",
      "4.370448\n",
      "4.378999\n",
      "4.255619\n",
      "4.187014\n",
      "4.547824\n",
      "5.1464524\n",
      "4.793499\n",
      "4.245529\n",
      "4.2263618\n",
      "4.189144\n",
      "4.2556214\n",
      "4.2774653\n",
      "4.2033205\n",
      "4.212135\n",
      "4.310278\n",
      "4.2548914\n",
      "4.120658\n",
      "4.1922274\n",
      "4.4311514\n",
      "4.186319\n",
      "4.1670666\n",
      "4.1775374\n",
      "4.1077495\n",
      "4.038469\n",
      "4.2280746\n",
      "4.210022\n",
      "4.207857\n",
      "4.2384706\n",
      "4.1314454\n",
      "4.170774\n",
      "3.9936473\n",
      "4.1679344\n",
      "4.1079607\n",
      "4.0399995\n",
      "4.2991195\n",
      "4.1547203\n",
      "4.1683517\n",
      "4.1830072\n",
      "4.222092\n",
      "4.1393027\n",
      "4.2010527\n",
      "4.1076794\n",
      "4.0903864\n",
      "3.9797409\n",
      "4.1051283\n",
      "4.1165414\n",
      "4.0133224\n",
      "4.075248\n",
      "3.8975117\n",
      "3.990652\n",
      "4.3057275\n",
      "4.000367\n",
      "4.1179743\n",
      "4.2144375\n",
      "4.044059\n",
      "4.13034\n",
      "4.0318656\n",
      "3.9818788\n",
      "3.969685\n",
      "3.9911742\n",
      "4.126415\n",
      "4.097704\n",
      "4.0329046\n",
      "3.9665055\n",
      "3.9559295\n",
      "4.1285086\n",
      "4.007395\n",
      "4.0539637\n",
      "4.070298\n",
      "3.8510563\n",
      "4.1116247\n",
      "3.8208165\n",
      "3.8816109\n",
      "4.0057087\n",
      "4.0208964\n",
      "4.040188\n",
      "3.951051\n",
      "4.02572\n",
      "3.8387237\n",
      "3.9987183\n",
      "3.9815152\n",
      "3.9457812\n",
      "3.9709141\n",
      "3.7767863\n",
      "3.9477983\n",
      "3.9956024\n",
      "3.8665824\n",
      "3.978435\n",
      "4.109281\n",
      "3.940211\n",
      "3.9617615\n",
      "3.9902287\n",
      "4.0057597\n",
      "3.9379013\n",
      "4.1375537\n",
      "4.068873\n",
      "4.022619\n",
      "3.9862454\n",
      "4.0434546\n",
      "3.990493\n",
      "4.0447993\n",
      "4.181815\n",
      "3.9876094\n",
      "3.9953911\n",
      "3.917078\n",
      "3.8941398\n",
      "4.0637197\n",
      "3.9756615\n",
      "3.9715304\n",
      "4.0648\n",
      "3.9283001\n",
      "3.998443\n",
      "4.0897145\n",
      "4.041599\n",
      "4.0704856\n",
      "3.9647915\n",
      "3.8885684\n",
      "4.0313044\n",
      "4.016282\n",
      "3.896953\n",
      "3.9483051\n",
      "3.9874191\n",
      "3.9926708\n",
      "4.0191865\n",
      "3.8456142\n",
      "3.9872937\n",
      "3.897293\n",
      "3.9712281\n",
      "4.016758\n",
      "3.9291527\n",
      "3.864843\n",
      "4.016217\n",
      "3.9522889\n",
      "3.9488482\n",
      "4.0156627\n",
      "3.9890828\n",
      "3.9073894\n",
      "3.9484239\n",
      "3.887528\n",
      "4.0038114\n",
      "3.8983264\n",
      "3.9097207\n",
      "3.952916\n",
      "3.946951\n",
      "3.9280841\n",
      "3.9977143\n",
      "4.1145906\n",
      "3.8771958\n",
      "3.9715028\n",
      "3.9678168\n",
      "3.9422848\n",
      "3.898404\n",
      "3.9106228\n",
      "3.9695466\n",
      "4.044688\n",
      "3.8813252\n",
      "3.8941293\n",
      "3.8965702\n",
      "3.9121928\n",
      "3.950425\n",
      "3.952003\n",
      "3.7074025\n",
      "3.9087448\n",
      "3.7475667\n",
      "4.170877\n",
      "4.1353483\n",
      "3.768479\n",
      "3.9977093\n",
      "4.138439\n",
      "3.9634843\n",
      "3.9434311\n",
      "3.939603\n",
      "3.988582\n",
      "3.8705616\n",
      "3.8574634\n",
      "4.051898\n",
      "3.8385518\n",
      "3.9361794\n",
      "3.860551\n",
      "3.833776\n",
      "3.9204853\n",
      "4.0113664\n",
      "3.8478336\n",
      "3.9370053\n",
      "3.7913837\n",
      "3.7728646\n",
      "3.8570747\n",
      "4.048233\n",
      "3.9142709\n",
      "3.8948789\n",
      "4.0105996\n",
      "3.8645172\n",
      "3.85057\n",
      "4.013501\n",
      "4.0007973\n",
      "3.881937\n",
      "3.93148\n",
      "3.7941356\n",
      "4.0907288\n",
      "3.9552045\n",
      "3.8527515\n",
      "3.8862724\n",
      "3.9264953\n",
      "3.9449246\n",
      "3.9320064\n",
      "3.8975015\n",
      "3.857127\n",
      "3.8549476\n",
      "3.9556937\n",
      "3.9005666\n",
      "3.7868729\n",
      "3.9169216\n",
      "3.979454\n",
      "3.8436172\n",
      "3.8351204\n",
      "3.9586143\n",
      "3.8402328\n",
      "3.839725\n",
      "3.89114\n",
      "3.8751404\n",
      "3.9803517\n",
      "4.0525656\n",
      "3.9085903\n",
      "3.8363075\n",
      "3.7468204\n",
      "3.8658485\n",
      "4.0333266\n",
      "3.8126435\n",
      "3.9515405\n",
      "3.9337966\n",
      "4.0236864\n",
      "3.8356903\n",
      "3.807647\n",
      "3.962522\n",
      "3.8351822\n",
      "3.993517\n",
      "3.9372506\n",
      "3.831035\n",
      "3.8832772\n",
      "3.7258859\n",
      "3.8402512\n",
      "3.8207004\n",
      "3.7356582\n",
      "3.878078\n",
      "3.9366043\n",
      "3.9416642\n",
      "3.8948383\n",
      "3.901294\n",
      "3.8479705\n",
      "3.867063\n",
      "3.839756\n",
      "3.7979648\n",
      "3.7983162\n",
      "3.9814587\n",
      "3.9229686\n",
      "3.7641346\n",
      "3.8948312\n",
      "3.8643966\n",
      "3.9067008\n",
      "3.7636847\n",
      "3.823166\n",
      "3.8226018\n",
      "3.866877\n",
      "3.9468734\n",
      "3.9307258\n",
      "3.9024081\n",
      "3.856371\n",
      "3.8188481\n",
      "3.845972\n",
      "3.9202487\n",
      "3.8953831\n",
      "3.867606\n",
      "3.796737\n",
      "3.8262577\n",
      "3.9762588\n",
      "3.8609843\n",
      "3.8208203\n",
      "3.9150467\n",
      "3.931689\n",
      "3.8309002\n",
      "3.7923424\n",
      "3.7603526\n",
      "3.8888671\n",
      "3.7602286\n",
      "3.845428\n",
      "3.7251523\n",
      "3.683349\n",
      "3.8023612\n",
      "3.8102937\n",
      "3.7345943\n",
      "3.8647246\n",
      "3.8574097\n",
      "3.825917\n",
      "3.7141185\n",
      "3.7999413\n",
      "Epoch: 0.00, Time: 90.23,  Loss: 3.90\n",
      "Perplexity: 46.95\n",
      "3.777659\n",
      "3.981427\n",
      "3.859148\n",
      "4.020317\n",
      "3.9426408\n",
      "3.7898815\n",
      "3.7425156\n",
      "3.7970862\n",
      "3.9020877\n",
      "3.9278984\n",
      "3.9249527\n",
      "3.8762715\n",
      "3.8282497\n",
      "3.824498\n",
      "3.7895055\n",
      "3.8291543\n",
      "3.7671838\n",
      "3.8317971\n",
      "3.8749988\n",
      "3.9456697\n",
      "3.8106964\n",
      "3.7481186\n",
      "3.7719204\n",
      "3.8546169\n",
      "3.8628192\n",
      "3.8189952\n",
      "3.7908216\n",
      "3.809632\n",
      "3.87114\n",
      "3.726596\n",
      "3.659644\n",
      "3.8567154\n",
      "3.636869\n",
      "3.8230507\n",
      "3.833327\n",
      "3.870298\n",
      "3.8151855\n",
      "3.7749915\n",
      "3.9774842\n",
      "3.950229\n",
      "3.7329946\n",
      "3.8092175\n",
      "3.7946057\n",
      "3.7440178\n",
      "3.7727838\n",
      "3.784512\n",
      "3.7234054\n",
      "3.7112002\n",
      "3.8429253\n",
      "3.7914166\n",
      "3.6364677\n",
      "3.7453876\n",
      "3.846711\n",
      "3.7756524\n",
      "3.8424797\n",
      "3.8370717\n",
      "3.784516\n",
      "3.7574134\n",
      "3.840915\n",
      "3.840528\n",
      "3.660791\n",
      "3.8580918\n",
      "3.810038\n",
      "3.719051\n",
      "3.7586894\n",
      "3.8224366\n",
      "3.7076867\n",
      "3.621167\n",
      "3.8327599\n",
      "3.9147499\n",
      "3.846067\n",
      "3.675553\n",
      "3.823666\n",
      "3.7308972\n",
      "3.7930417\n",
      "3.8359516\n",
      "3.7243187\n",
      "3.6814187\n",
      "3.7758331\n",
      "3.802536\n",
      "3.7346756\n",
      "3.7622147\n",
      "3.6505578\n",
      "3.678809\n",
      "3.80023\n",
      "3.8124573\n",
      "3.7972178\n",
      "3.859272\n",
      "3.8084106\n",
      "3.8311648\n",
      "3.758258\n",
      "3.7025547\n",
      "3.6044827\n",
      "3.747094\n",
      "3.8141658\n",
      "3.7802615\n",
      "3.7035522\n",
      "3.6055336\n",
      "3.7693934\n",
      "3.7395554\n",
      "3.8249886\n",
      "3.7941613\n",
      "3.7655957\n",
      "3.629673\n",
      "3.85794\n",
      "3.597543\n",
      "3.6253567\n",
      "3.7563987\n",
      "3.7762177\n",
      "3.7556224\n",
      "3.689878\n",
      "3.7538192\n",
      "3.666705\n",
      "3.697989\n",
      "3.6915183\n",
      "3.6742523\n",
      "3.7044857\n",
      "3.6115272\n",
      "3.6722045\n",
      "3.7129626\n",
      "3.593628\n",
      "3.634121\n",
      "3.7676098\n",
      "3.700774\n",
      "3.8062062\n",
      "3.7686493\n",
      "3.7634442\n",
      "3.7275422\n",
      "3.991047\n",
      "3.833512\n",
      "3.7690525\n",
      "3.6587777\n",
      "3.717992\n",
      "3.7683506\n",
      "3.7847342\n",
      "3.8865936\n",
      "3.663962\n",
      "3.7801566\n",
      "3.6727433\n",
      "3.730739\n",
      "3.7555306\n",
      "3.7391381\n",
      "3.6955209\n",
      "3.7522302\n",
      "3.7089248\n",
      "3.6946957\n",
      "3.8369548\n",
      "3.8464081\n",
      "3.8028884\n",
      "3.7178226\n",
      "3.6437354\n",
      "3.8093152\n",
      "3.8262138\n",
      "3.6639225\n",
      "3.7203858\n",
      "3.8120396\n",
      "3.8094463\n",
      "3.835355\n",
      "3.628385\n",
      "3.7604852\n",
      "3.683353\n",
      "3.7705815\n",
      "3.8096051\n",
      "3.7289302\n",
      "3.5919557\n",
      "3.8005435\n",
      "3.6906517\n",
      "3.7368648\n",
      "3.839756\n",
      "3.781463\n",
      "3.737384\n",
      "3.6607513\n",
      "3.6331413\n",
      "3.8218136\n",
      "3.8067513\n",
      "3.7056656\n",
      "3.773053\n",
      "3.74429\n",
      "3.6998436\n",
      "3.7543063\n",
      "3.7671323\n",
      "3.7398262\n",
      "3.7750783\n",
      "3.7079911\n",
      "3.7236106\n",
      "3.7475288\n",
      "3.6871324\n",
      "3.5833213\n",
      "3.7277617\n",
      "3.7083638\n",
      "3.719337\n",
      "3.6669025\n",
      "3.7455032\n",
      "3.6958477\n",
      "3.7247186\n",
      "3.6757646\n",
      "3.5869288\n",
      "3.589318\n",
      "3.9203048\n",
      "3.92666\n",
      "3.5844479\n",
      "3.7158604\n",
      "3.8538156\n",
      "3.8099713\n",
      "3.6320012\n",
      "3.7983317\n",
      "3.7501168\n",
      "3.676331\n",
      "3.6193547\n",
      "3.7984989\n",
      "3.6103942\n",
      "3.7098007\n",
      "3.6975625\n",
      "3.6589592\n",
      "3.6866524\n",
      "3.7673678\n",
      "3.732387\n",
      "3.7384079\n",
      "3.6711087\n",
      "3.6810284\n",
      "3.5869033\n",
      "3.8849874\n",
      "3.6623218\n",
      "3.7822666\n",
      "3.809557\n",
      "3.520163\n",
      "3.6559455\n",
      "3.7676742\n",
      "3.726911\n",
      "3.6670883\n",
      "3.7144952\n",
      "3.601654\n",
      "3.8620849\n",
      "3.7844684\n",
      "3.6909459\n",
      "3.7508254\n",
      "3.6805482\n",
      "3.8070056\n",
      "3.747745\n",
      "3.7343001\n",
      "3.6196792\n",
      "3.609319\n",
      "3.8265655\n",
      "3.6926754\n",
      "3.7209544\n",
      "3.750711\n",
      "3.865663\n",
      "3.7298088\n",
      "3.651712\n",
      "3.796569\n",
      "3.6860194\n",
      "3.655569\n",
      "3.6858602\n",
      "3.7461014\n",
      "3.7859867\n",
      "3.814122\n",
      "3.6970844\n",
      "3.6201031\n",
      "3.6283278\n",
      "3.6651306\n",
      "3.818898\n",
      "3.588031\n",
      "3.7599754\n",
      "3.7098076\n",
      "3.8748074\n",
      "3.7262263\n",
      "3.7145996\n",
      "3.8007464\n",
      "3.699119\n",
      "3.7435462\n",
      "3.7593937\n",
      "3.6635544\n",
      "3.6574497\n",
      "3.571708\n",
      "3.7173848\n",
      "3.6693711\n",
      "3.6247525\n",
      "3.7744968\n",
      "3.8078685\n",
      "3.7725158\n",
      "3.705218\n",
      "3.7671456\n",
      "3.7043571\n",
      "3.791082\n",
      "3.705669\n",
      "3.665257\n",
      "3.6658108\n",
      "3.8076196\n",
      "3.7624695\n",
      "3.5779824\n",
      "3.7295632\n",
      "3.7029908\n",
      "3.7714787\n",
      "3.6764464\n",
      "3.7448235\n",
      "3.585837\n",
      "3.7772102\n",
      "3.70614\n",
      "3.766272\n",
      "3.8679252\n",
      "3.7166991\n",
      "3.6825283\n",
      "3.7063987\n",
      "3.7253277\n",
      "3.6811671\n",
      "3.7454858\n",
      "3.6438546\n",
      "3.610876\n",
      "3.8873456\n",
      "3.6856315\n",
      "3.6976058\n",
      "3.7537932\n",
      "3.784181\n",
      "3.654386\n",
      "3.691393\n",
      "3.6141155\n",
      "3.7599335\n",
      "3.5864804\n",
      "3.6470523\n",
      "3.6743836\n",
      "3.671561\n",
      "3.6608748\n",
      "3.6245701\n",
      "3.5186515\n",
      "3.7993684\n",
      "3.6737633\n",
      "3.647164\n",
      "3.6277943\n",
      "3.6782753\n",
      "Epoch: 1.00, Time: 93.22,  Loss: 3.75\n",
      "3.6423821\n",
      "3.8490214\n",
      "3.6725407\n",
      "3.833561\n",
      "3.6191006\n",
      "3.6083164\n",
      "3.6891053\n",
      "3.6101203\n",
      "3.712604\n",
      "3.7750797\n",
      "3.7074094\n",
      "3.7424564\n",
      "3.7186866\n",
      "3.6944573\n",
      "3.631794\n",
      "3.695435\n",
      "3.636354\n",
      "3.717411\n",
      "3.7465553\n",
      "3.8334866\n",
      "3.5964336\n",
      "3.5777066\n",
      "3.6988525\n",
      "3.7507434\n",
      "3.7225296\n",
      "3.5976663\n",
      "3.7078648\n",
      "3.663591\n",
      "3.6929348\n",
      "3.70091\n",
      "3.6583228\n",
      "3.7256827\n",
      "3.575782\n",
      "3.7357147\n",
      "3.7482147\n",
      "3.7854252\n",
      "3.6575036\n",
      "3.7155023\n",
      "3.827376\n",
      "3.7883327\n",
      "3.6113803\n",
      "3.6888912\n",
      "3.6364841\n",
      "3.6428158\n",
      "3.6961122\n",
      "3.6882026\n",
      "3.6802113\n",
      "3.5400124\n",
      "3.7275455\n",
      "3.7111793\n",
      "3.687374\n",
      "3.615397\n",
      "3.8305538\n",
      "3.7564735\n",
      "3.7310066\n",
      "3.7431533\n",
      "3.6424823\n",
      "3.6462429\n",
      "3.705562\n",
      "3.6811821\n",
      "3.5976212\n",
      "3.7993457\n",
      "3.6299145\n",
      "3.6536992\n",
      "3.618773\n",
      "3.7216823\n",
      "3.6681297\n",
      "3.6030571\n",
      "3.714229\n",
      "3.7149956\n",
      "3.6356869\n",
      "3.6177773\n",
      "3.6755836\n",
      "3.6793492\n",
      "3.7241817\n",
      "3.7382514\n",
      "3.5879498\n",
      "3.5668976\n",
      "3.714264\n",
      "3.6905518\n",
      "3.5545056\n",
      "3.63539\n",
      "3.557085\n",
      "3.6484718\n",
      "3.850846\n",
      "3.7353244\n",
      "3.7243125\n",
      "3.7260754\n",
      "3.6520932\n",
      "3.8042698\n",
      "3.6787858\n",
      "3.6078897\n",
      "3.5890725\n",
      "3.737447\n",
      "3.6678421\n",
      "3.6134346\n",
      "3.6782563\n",
      "3.4683251\n",
      "3.6151364\n",
      "3.6531155\n",
      "3.6846128\n",
      "3.7562587\n",
      "3.6816134\n",
      "3.5273285\n",
      "3.7316413\n",
      "3.5367255\n",
      "3.5673378\n",
      "3.6683292\n",
      "3.6696072\n",
      "3.5899029\n",
      "3.5731301\n",
      "3.6867242\n",
      "3.5456536\n",
      "3.6412556\n",
      "3.6285176\n",
      "3.6506438\n",
      "3.6588645\n",
      "3.5433686\n",
      "3.571603\n",
      "3.6701787\n",
      "3.4893365\n",
      "3.5712104\n",
      "3.6963143\n",
      "3.6013157\n",
      "3.6565242\n",
      "3.6470997\n",
      "3.674598\n",
      "3.582611\n",
      "3.8826165\n",
      "3.7177742\n",
      "3.6111324\n",
      "3.574416\n",
      "3.556418\n",
      "3.706405\n",
      "3.6825497\n",
      "3.854646\n",
      "3.6304977\n",
      "3.6314952\n",
      "3.5941205\n",
      "3.6260471\n",
      "3.625909\n",
      "3.698518\n",
      "3.5688558\n",
      "3.7493248\n",
      "3.64114\n",
      "3.5973172\n",
      "3.7823129\n",
      "3.7030098\n",
      "3.7975686\n",
      "3.5652943\n",
      "3.5528398\n",
      "3.7107627\n",
      "3.7457795\n",
      "3.57871\n",
      "3.6121306\n",
      "3.637956\n",
      "3.6837268\n",
      "3.6833751\n",
      "3.5953426\n",
      "3.591533\n",
      "3.5891972\n",
      "3.586657\n",
      "3.6907158\n",
      "3.5993218\n",
      "3.6420045\n",
      "3.745713\n",
      "3.6080966\n",
      "3.621563\n",
      "3.7155426\n",
      "3.6416085\n",
      "3.6292152\n",
      "3.5878997\n",
      "3.4972615\n",
      "3.7438617\n",
      "3.6306489\n",
      "3.62323\n",
      "3.6813233\n",
      "3.6033196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.61334\n",
      "3.643412\n",
      "3.742365\n",
      "3.6269886\n",
      "3.6102433\n",
      "3.5786102\n",
      "3.5949948\n",
      "3.6666284\n",
      "3.6070457\n",
      "3.5942636\n",
      "3.6784482\n",
      "3.6857471\n",
      "3.62155\n",
      "3.6090817\n",
      "3.6326137\n",
      "3.6653435\n",
      "3.5948634\n",
      "3.5928218\n",
      "3.5155098\n",
      "3.5341084\n",
      "3.7249234\n",
      "3.8118472\n",
      "3.4735093\n",
      "3.6714077\n",
      "3.7636433\n",
      "3.7590127\n",
      "3.6077123\n",
      "3.724278\n",
      "3.6936889\n",
      "3.5271218\n",
      "3.5646882\n",
      "3.7233834\n",
      "3.5090365\n",
      "3.6663532\n",
      "3.582542\n",
      "3.594579\n",
      "3.5936196\n",
      "3.6765742\n",
      "3.623345\n",
      "3.5867755\n",
      "3.558548\n",
      "3.6308064\n",
      "3.5472329\n",
      "3.7154994\n",
      "3.6727226\n",
      "3.6493888\n",
      "3.7159762\n",
      "3.5534186\n",
      "3.6026845\n",
      "3.69044\n",
      "3.6279721\n",
      "3.614453\n",
      "3.6128547\n",
      "3.575107\n",
      "3.7331283\n",
      "3.6972938\n",
      "3.6730857\n",
      "3.6319084\n",
      "3.6509414\n",
      "3.7335098\n",
      "3.6913924\n",
      "3.6239052\n",
      "3.544989\n",
      "3.5987067\n",
      "3.651792\n",
      "3.660118\n",
      "3.6290488\n",
      "3.6209805\n",
      "3.7093391\n",
      "3.6154819\n",
      "3.5838382\n",
      "3.646777\n",
      "3.5448864\n",
      "3.6452394\n",
      "3.509856\n",
      "3.6227226\n",
      "3.648582\n",
      "3.7298806\n",
      "3.6311371\n",
      "3.5810673\n",
      "3.5327764\n",
      "3.6230507\n",
      "3.7241974\n",
      "3.5833907\n",
      "3.677082\n",
      "3.6396255\n",
      "3.7697308\n",
      "3.6717458\n",
      "3.5915933\n",
      "3.7259097\n",
      "3.6938825\n",
      "3.7029343\n",
      "3.6557014\n",
      "3.587111\n",
      "3.5233421\n",
      "3.5504947\n",
      "3.5925224\n",
      "3.6441612\n",
      "3.5414085\n",
      "3.6674962\n",
      "3.6699517\n",
      "3.7582583\n",
      "3.7195625\n",
      "3.6755757\n",
      "3.7095075\n",
      "3.5061307\n",
      "3.6626205\n",
      "3.5967941\n",
      "3.6246467\n",
      "3.792659\n",
      "3.6289356\n",
      "3.552073\n",
      "3.6830993\n",
      "3.618866\n",
      "3.6654181\n",
      "3.5909793\n",
      "3.7231014\n",
      "3.5136695\n",
      "3.673505\n",
      "3.6644683\n",
      "3.627318\n",
      "3.6576202\n",
      "3.6368623\n",
      "3.6997788\n",
      "3.6079037\n",
      "3.659787\n",
      "3.6136832\n",
      "3.5894592\n",
      "3.594421\n",
      "3.6257164\n",
      "3.7124095\n",
      "3.6480536\n",
      "3.6377017\n",
      "3.730048\n",
      "3.706459\n",
      "3.602117\n",
      "3.5674698\n",
      "3.562476\n",
      "3.675898\n",
      "3.5009408\n",
      "3.5738614\n",
      "3.5155818\n",
      "3.5949543\n",
      "3.5624473\n",
      "3.6362178\n",
      "3.5530357\n",
      "3.652541\n",
      "3.5919957\n",
      "3.627997\n",
      "3.5579314\n",
      "3.5308247\n",
      "Epoch: 2.00, Time: 97.04,  Loss: 3.69\n",
      "Perplexity: 38.73\n",
      "3.556221\n",
      "3.723414\n",
      "3.6316504\n",
      "3.7828987\n",
      "3.5949311\n",
      "3.5905137\n",
      "3.5953827\n",
      "3.5750797\n",
      "3.6543021\n",
      "3.6814442\n",
      "3.712088\n",
      "3.6834111\n",
      "3.6197941\n",
      "3.6208878\n",
      "3.6287835\n",
      "3.5921528\n",
      "3.5788937\n",
      "3.6229706\n",
      "3.609301\n",
      "3.6941714\n",
      "3.6367276\n",
      "3.6228664\n",
      "3.6001308\n",
      "3.6852958\n",
      "3.6970277\n",
      "3.5555406\n",
      "3.6503587\n",
      "3.6054718\n",
      "3.5369034\n",
      "3.6770089\n",
      "3.5695865\n",
      "3.7306466\n",
      "3.471292\n",
      "3.69586\n",
      "3.6387074\n",
      "3.7253075\n",
      "3.6247993\n",
      "3.583348\n",
      "3.697766\n",
      "3.6594481\n",
      "3.542173\n",
      "3.5937514\n",
      "3.5714746\n",
      "3.5033584\n",
      "3.5412228\n",
      "3.5634472\n",
      "3.5682251\n",
      "3.5977905\n",
      "3.6105976\n",
      "3.6358466\n",
      "3.5383732\n",
      "3.4548142\n",
      "3.684962\n",
      "3.695903\n",
      "3.6428425\n",
      "3.6500669\n",
      "3.616391\n",
      "3.6188996\n",
      "3.6547718\n",
      "3.6377907\n",
      "3.486562\n",
      "3.663121\n",
      "3.6332035\n",
      "3.6761684\n",
      "3.6042073\n",
      "3.633452\n",
      "3.593537\n",
      "3.510796\n",
      "3.675377\n",
      "3.673284\n",
      "3.627955\n",
      "3.4936383\n",
      "3.6959846\n",
      "3.6029618\n",
      "3.55873\n",
      "3.6413784\n",
      "3.5947697\n",
      "3.601984\n",
      "3.6336274\n",
      "3.6261754\n",
      "3.5509968\n",
      "3.625032\n",
      "3.532029\n",
      "3.515335\n",
      "3.6620784\n",
      "3.6601224\n",
      "3.6405702\n",
      "3.6394467\n",
      "3.5951715\n",
      "3.7241797\n",
      "3.582985\n",
      "3.574923\n",
      "3.5192504\n",
      "3.6637752\n",
      "3.675716\n",
      "3.5286517\n",
      "3.606762\n",
      "3.49598\n",
      "3.5809524\n",
      "3.6313019\n",
      "3.6722443\n",
      "3.638348\n",
      "3.6123295\n",
      "3.5302186\n",
      "3.6642787\n",
      "3.4392471\n",
      "3.495748\n",
      "3.6234028\n",
      "3.635617\n",
      "3.525577\n",
      "3.4489791\n",
      "3.594963\n",
      "3.4847534\n",
      "3.5588868\n",
      "3.5898337\n",
      "3.5207946\n",
      "3.6145577\n",
      "3.4618058\n",
      "3.5290577\n",
      "3.6041164\n",
      "3.5069385\n",
      "3.5882561\n",
      "3.6223588\n",
      "3.5868773\n",
      "3.540155\n",
      "3.5432804\n",
      "3.6398556\n",
      "3.5754037\n",
      "3.8087254\n",
      "3.6252525\n",
      "3.6411526\n",
      "3.540313\n",
      "3.5983589\n",
      "3.6718154\n",
      "3.701151\n",
      "3.7254288\n",
      "3.5900264\n",
      "3.5300262\n",
      "3.5561066\n",
      "3.5696702\n",
      "3.5944\n",
      "3.6029866\n",
      "3.4419487\n",
      "3.6863358\n",
      "3.542476\n",
      "3.4390411\n",
      "3.7549732\n",
      "3.555098\n",
      "3.6854587\n",
      "3.5386314\n",
      "3.55186\n",
      "3.6037574\n",
      "3.6923676\n",
      "3.5262008\n",
      "3.6335678\n",
      "3.6111863\n",
      "3.6205525\n",
      "3.5909362\n",
      "3.5579453\n",
      "3.4835663\n",
      "3.5712943\n",
      "3.596674\n",
      "3.607048\n",
      "3.48228\n",
      "3.5238862\n",
      "3.547151\n",
      "3.5932984\n",
      "3.5449498\n",
      "3.640255\n",
      "3.5434906\n",
      "3.5553734\n",
      "3.5829926\n",
      "3.6153827\n",
      "3.5975842\n",
      "3.6517944\n",
      "3.5674546\n",
      "3.653646\n",
      "3.5856261\n",
      "3.4820647\n",
      "3.5863335\n",
      "3.6243854\n",
      "3.5461335\n",
      "3.6262405\n",
      "3.5383134\n",
      "3.563528\n",
      "3.6279202\n",
      "3.6140475\n",
      "3.5404363\n",
      "3.6253715\n",
      "3.6092331\n",
      "3.4810257\n",
      "3.511446\n",
      "3.5768554\n",
      "3.5559146\n",
      "3.656609\n",
      "3.470714\n",
      "3.5273826\n",
      "3.5291405\n",
      "3.6999822\n",
      "3.7026813\n",
      "3.4664397\n",
      "3.5924466\n",
      "3.712059\n",
      "3.6882267\n",
      "3.5542328\n",
      "3.6383033\n",
      "3.5699844\n",
      "3.5810394\n",
      "3.5066383\n",
      "3.6539083\n",
      "3.531107\n",
      "3.573666\n",
      "3.616563\n",
      "3.4873688\n",
      "3.5285249\n",
      "3.5895915\n",
      "3.5819163\n",
      "3.5400863\n",
      "3.444746\n",
      "3.5756347\n",
      "3.5070114\n",
      "3.7113717\n",
      "3.5958512\n",
      "3.6355565\n",
      "3.684389\n",
      "3.5032678\n",
      "3.5591407\n",
      "3.7043242\n",
      "3.5813186\n",
      "3.5625434\n",
      "3.530695\n",
      "3.4733841\n",
      "3.6887493\n",
      "3.6787877\n",
      "3.6029396\n",
      "3.563237\n",
      "3.6234295\n",
      "3.701715\n",
      "3.6250706\n",
      "3.5496757\n",
      "3.5884354\n",
      "3.4774537\n",
      "3.675591\n",
      "3.6749005\n",
      "3.6062615\n",
      "3.573798\n",
      "3.6371613\n",
      "3.556266\n",
      "3.628421\n",
      "3.6352665\n",
      "3.5790951\n",
      "3.53044\n",
      "3.597489\n",
      "3.526638\n",
      "3.6193702\n",
      "3.6436858\n",
      "3.6338165\n",
      "3.4760852\n",
      "3.5219684\n",
      "3.558844\n",
      "3.695209\n",
      "3.4997582\n",
      "3.6183503\n",
      "3.5537198\n",
      "3.6353478\n",
      "3.5602188\n",
      "3.5500379\n",
      "3.66483\n",
      "3.544072\n",
      "3.644656\n",
      "3.6208854\n",
      "3.5382915\n",
      "3.4640167\n",
      "3.5571315\n",
      "3.6136327\n",
      "3.540875\n",
      "3.5201275\n",
      "3.5753074\n",
      "3.638867\n",
      "3.6825924\n",
      "3.5396526\n",
      "3.6185765\n",
      "3.6149154\n",
      "3.6118214\n",
      "3.5390213\n",
      "3.4919772\n",
      "3.5664947\n",
      "3.7415168\n",
      "3.6018124\n",
      "3.5357883\n",
      "3.566079\n",
      "3.6135573\n",
      "3.6356895\n",
      "3.5542953\n",
      "3.6421769\n",
      "3.4785569\n",
      "3.577219\n",
      "3.5644073\n",
      "3.6015563\n",
      "3.7189603\n",
      "3.6144798\n",
      "3.6402993\n",
      "3.5327191\n",
      "3.6387913\n",
      "3.4976737\n",
      "3.602179\n",
      "3.5372262\n",
      "3.5574253\n",
      "3.67338\n",
      "3.5947003\n",
      "3.5606132\n",
      "3.7007568\n",
      "3.664016\n",
      "3.5127442\n",
      "3.481816\n",
      "3.5767632\n",
      "3.6540794\n",
      "3.408671\n",
      "3.4651809\n",
      "3.4977946\n",
      "3.5509846\n",
      "3.559011\n",
      "3.535809\n",
      "3.5059707\n",
      "3.6237113\n",
      "3.6081905\n",
      "3.5574374\n",
      "3.5397403\n",
      "3.51235\n",
      "Epoch: 3.00, Time: 86.02,  Loss: 3.63\n",
      "3.4902472\n",
      "3.7320728\n",
      "3.585687\n",
      "3.7258022\n",
      "3.5361626\n",
      "3.4872098\n",
      "3.523206\n",
      "3.4204133\n",
      "3.6097398\n",
      "3.6050897\n",
      "3.7168584\n",
      "3.6024976\n",
      "3.535149\n",
      "3.630099\n",
      "3.6269183\n",
      "3.5594356\n",
      "3.5292888\n",
      "3.543237\n",
      "3.616703\n",
      "3.693971\n",
      "3.6213613\n",
      "3.5801625\n",
      "3.6190314\n",
      "3.623097\n",
      "3.6843388\n",
      "3.5859828\n",
      "3.608661\n",
      "3.5361218\n",
      "3.5490997\n",
      "3.6337247\n",
      "3.4757943\n",
      "3.7045834\n",
      "3.488011\n",
      "3.576199\n",
      "3.610551\n",
      "3.651336\n",
      "3.583569\n",
      "3.6264522\n",
      "3.6636143\n",
      "3.61781\n",
      "3.528706\n",
      "3.557942\n",
      "3.575098\n",
      "3.4807963\n",
      "3.5811515\n",
      "3.5278773\n",
      "3.4925456\n",
      "3.5631576\n",
      "3.5985074\n",
      "3.5627613\n",
      "3.4939926\n",
      "3.4497674\n",
      "3.608296\n",
      "3.6313336\n",
      "3.5842712\n",
      "3.6642334\n",
      "3.5314355\n",
      "3.5987556\n",
      "3.5664546\n",
      "3.5436096\n",
      "3.4596508\n",
      "3.6544502\n",
      "3.5228267\n",
      "3.58273\n",
      "3.527774\n",
      "3.6252007\n",
      "3.5521328\n",
      "3.482853\n",
      "3.5996766\n",
      "3.6162572\n",
      "3.630156\n",
      "3.4814484\n",
      "3.649836\n",
      "3.5535731\n",
      "3.538622\n",
      "3.588985\n",
      "3.5142937\n",
      "3.4873912\n",
      "3.5567021\n",
      "3.572226\n",
      "3.4811516\n",
      "3.5725713\n",
      "3.5604205\n",
      "3.5484653\n",
      "3.6756473\n",
      "3.6138732\n",
      "3.5742164\n",
      "3.6204052\n",
      "3.5276494\n",
      "3.648866\n",
      "3.5393136\n",
      "3.4433284\n",
      "3.4782977\n",
      "3.595153\n",
      "3.607783\n",
      "3.537091\n",
      "3.532165\n",
      "3.499585\n",
      "3.484066\n",
      "3.6350067\n",
      "3.5545037\n",
      "3.5563188\n",
      "3.6287422\n",
      "3.4878812\n",
      "3.552014\n",
      "3.434056\n",
      "3.485344\n",
      "3.6409879\n",
      "3.5896218\n",
      "3.554198\n",
      "3.4673638\n",
      "3.5198739\n",
      "3.456284\n",
      "3.445064\n",
      "3.498901\n",
      "3.5158544\n",
      "3.5538344\n",
      "3.5124078\n",
      "3.4742572\n",
      "3.574714\n",
      "3.4178817\n",
      "3.485249\n",
      "3.6254044\n",
      "3.4393718\n",
      "3.5572934\n",
      "3.5622468\n",
      "3.6326604\n",
      "3.561315\n",
      "3.7784405\n",
      "3.6284916\n",
      "3.5791485\n",
      "3.5011792\n",
      "3.6321254\n",
      "3.5361679\n",
      "3.6664538\n",
      "3.6816196\n",
      "3.5003018\n",
      "3.5221162\n",
      "3.55996\n",
      "3.5504253\n",
      "3.5735843\n",
      "3.5313423\n",
      "3.4436316\n",
      "3.6045632\n",
      "3.5235734\n",
      "3.493573\n",
      "3.6737156\n",
      "3.6056244\n",
      "3.6137989\n",
      "3.5329845\n",
      "3.3753746\n",
      "3.6419463\n",
      "3.6595352\n",
      "3.5266995\n",
      "3.6114826\n",
      "3.5922024\n",
      "3.6628177\n",
      "3.5842423\n",
      "3.509626\n",
      "3.5214677\n",
      "3.5544293\n",
      "3.5253615\n",
      "3.5966542\n",
      "3.439576\n",
      "3.4978814\n",
      "3.609736\n",
      "3.5492146\n",
      "3.5829487\n",
      "3.632848\n",
      "3.5719705\n",
      "3.5182998\n",
      "3.518966\n",
      "3.5045846\n",
      "3.6314945\n",
      "3.3794696\n",
      "3.5636394\n",
      "3.5887084\n",
      "3.5720534\n",
      "3.4153638\n",
      "3.5665412\n",
      "3.6764839\n",
      "3.5607963\n",
      "3.5703347\n",
      "3.4923291\n",
      "3.5473225\n",
      "3.5715535\n",
      "3.5572934\n",
      "3.4289832\n",
      "3.6065419\n",
      "3.5668762\n",
      "3.5616004\n",
      "3.4941075\n",
      "3.5548217\n",
      "3.6380324\n",
      "3.5392342\n",
      "3.4973717\n",
      "3.4501579\n",
      "3.5373738\n",
      "3.7231762\n",
      "3.6981826\n",
      "3.4356194\n",
      "3.5333314\n",
      "3.6380038\n",
      "3.6263309\n",
      "3.549113\n",
      "3.5497952\n",
      "3.5007026\n",
      "3.5156503\n",
      "3.4537177\n",
      "3.6095386\n",
      "3.4407902\n",
      "3.5678837\n",
      "3.500089\n",
      "3.4808853\n",
      "3.4542508\n",
      "3.5371532\n",
      "3.53029\n",
      "3.5090299\n",
      "3.5013452\n",
      "3.5145168\n",
      "3.4340866\n",
      "3.6582828\n",
      "3.586766\n",
      "3.5426548\n",
      "3.6137192\n",
      "3.5078301\n",
      "3.4953103\n",
      "3.6912236\n",
      "3.5134933\n",
      "3.5371404\n",
      "3.4558628\n",
      "3.556893\n",
      "3.631825\n",
      "3.5713775\n",
      "3.521094\n",
      "3.566297\n",
      "3.5455296\n",
      "3.5859637\n",
      "3.5665176\n",
      "3.4293606\n",
      "3.4880924\n",
      "3.5233822\n",
      "3.6005158\n",
      "3.5766053\n",
      "3.5231235\n",
      "3.5610967\n",
      "3.679224\n",
      "3.4768796\n",
      "3.5578911\n",
      "3.594045\n",
      "3.563331\n",
      "3.5521476\n",
      "3.4744313\n",
      "3.5951772\n",
      "3.625827\n",
      "3.5808964\n",
      "3.4967697\n",
      "3.5333543\n",
      "3.4354215\n",
      "3.5806694\n",
      "3.6457527\n",
      "3.4350312\n",
      "3.6237202\n",
      "3.537114\n",
      "3.5891452\n",
      "3.5428855\n",
      "3.5220075\n",
      "3.6788476\n",
      "3.5748696\n",
      "3.578174\n",
      "3.5694432\n",
      "3.528274\n",
      "3.4641452\n",
      "3.4548206\n",
      "3.525521\n",
      "3.4611497\n",
      "3.4284186\n",
      "3.5454056\n",
      "3.631621\n",
      "3.6819901\n",
      "3.6306412\n",
      "3.5308106\n",
      "3.588665\n",
      "3.5035865\n",
      "3.4891777\n",
      "3.4437878\n",
      "3.538808\n",
      "3.6168447\n",
      "3.5650413\n",
      "3.4646118\n",
      "3.5750096\n",
      "3.5827415\n",
      "3.6470597\n",
      "3.5398746\n",
      "3.574957\n",
      "3.498471\n",
      "3.5352879\n",
      "3.5190558\n",
      "3.6140258\n",
      "3.6853263\n",
      "3.5937753\n",
      "3.5522652\n",
      "3.485066\n",
      "3.5819156\n",
      "3.5104947\n",
      "3.482005\n",
      "3.5176868\n",
      "3.5653865\n",
      "3.6363614\n",
      "3.516133\n",
      "3.5547757\n",
      "3.6123428\n",
      "3.6172223\n",
      "3.4900317\n",
      "3.4188983\n",
      "3.556741\n",
      "3.6394718\n",
      "3.4933267\n",
      "3.4315174\n",
      "3.5076816\n",
      "3.5093894\n",
      "3.512794\n",
      "3.5151258\n",
      "3.3799949\n",
      "3.5799592\n",
      "3.559911\n",
      "3.50493\n",
      "3.3910804\n",
      "3.453212\n",
      "Epoch: 4.00, Time: 85.93,  Loss: 3.65\n",
      "Perplexity: 38.10\n"
     ]
    }
   ],
   "source": [
    "batch_idx = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter(conf.summary_path, graph=sess.graph)\n",
    "\n",
    "    #if os.path.exists(conf.ckpt_file):\n",
    "        #saver.restore(sess, conf.ckpt_file)\n",
    "        #print(\"Model Restored\")\n",
    "\n",
    "    for i in np.arange(conf.epochs):\n",
    "        start = time.time()\n",
    "        for j in np.arange(conf.num_batches):\n",
    "        #for j in np.arange(21):\n",
    "            inputs, labels, batch_idx = dh.get_batch(x_batches, y_batches, batch_idx)\n",
    "            _, l = sess.run([model.optimizer, model.loss], feed_dict={model.X:inputs, model.y:labels})\n",
    "            if j%100 == 0:\n",
    "                print(l)\n",
    "        end = time.time()\n",
    "        print(\"Epoch: %.2f, Time: %.2f,  Loss: %.2f\"%(i, end-start, l))\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            perp = sess.run(model.perplexity, feed_dict={model.X:inputs, model.y:labels})\n",
    "            print(\"Perplexity: %.2f\"%perp)\n",
    "            saver.save(sess, conf.ckpt_file)\n",
    "\n",
    "        summaries = sess.run(model.merged_summary_op, feed_dict={model.X:inputs, model.y:labels})\n",
    "        summary_writer.add_summary(summaries, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "In this part, we need to use other datasets for sentiment analysis, like the one of SemEval2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use spacy tokenize sentences into words\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en\n",
    "#import spacy\n",
    "#nlp = spacy.load('en')\n",
    "#def sent_split(sent):\n",
    "    #words = []\n",
    "    #sent = nlp(sent.strip())\n",
    "    #for w in sent:\n",
    "        #words.append(w.text.lower())\n",
    "    #return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = 'data/semeval/downloaded.tsv'\n",
    "file_dev = 'data/semeval/dev_downloaded.tsv'\n",
    "file_test = 'data/semeval/test.txt'\n",
    "with open(file_train) as f:\n",
    "    tweets_train = f.readlines()\n",
    "with open(file_dev) as f:\n",
    "    tweets_dev = f.readlines()\n",
    "with open(file_test) as f:\n",
    "    tweets_test = f.readlines()\n",
    "    \n",
    "\n",
    "\n",
    "#Filter empty tweets\n",
    "def is_available(text):\n",
    "    if 'Not Available' in text:\n",
    "        return False\n",
    "    if '\\t\"objective' in text:\n",
    "        return False\n",
    "    if '\\t\"neutral' in text:\n",
    "        return False\n",
    "    if '\\tobjective' in text:\n",
    "        return False\n",
    "    if '\\tneutral' in text:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = list(filter(is_available, tweets_train))\n",
    "tweets_dev = list(filter(is_available, tweets_dev))\n",
    "tweets_test = list(filter(is_available, tweets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = [item.split('\\t') for item in tweets_train]\n",
    "tweets_dev = [item.split('\\t') for item in tweets_dev]\n",
    "tweets_test = [item.split('\\t') for item in tweets_test]\n",
    "_, _, y_train, text_train = list(zip(*tweets_train))\n",
    "_, _, y_dev, text_dev = list(zip(*tweets_dev))\n",
    "_, _, y_test, text_test = list(zip(*tweets_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, y_train = list(text_train), list(y_train)\n",
    "text_dev, y_dev = list(text_dev), list(y_dev)\n",
    "text_test, y_test = list(text_test), list(y_test)\n",
    "y_test = ['\"' + item + '\"' for item in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the labels to numbers\n",
    "from sklearn import preprocessing\n",
    "label_encode = preprocessing.LabelEncoder()  # 建立模型\n",
    "y_train = label_encode.fit_transform(y_train)\n",
    "y_dev = label_encode.transform(y_dev)\n",
    "y_test = label_encode.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sents_handler import generate_samples\n",
    "import numpy as np\n",
    "\n",
    "train_gs = generate_samples(np.array(text_train), np.array(y_train), word_to_idx, 20, True)\n",
    "#sent_vecs, sent_labels, lengths = train_gs.generate(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/vocab2000_embed200_filters64_batch32_layers3_block3_fdim5/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess_lm = tf.Session()\n",
    "saver.restore(sess_lm, conf.ckpt_file)\n",
    "def sent2vec(inputs, sess):\n",
    "    '''Get word representations'''\n",
    "    #Get the contextualized representation\n",
    "    #train_gs = generate_samples(np.array(text_train), np.array(y_train), word_to_idx, 20, False)\n",
    "    #sent_vecs, sent_labels, lengths = train_gs.generate(32)\n",
    "    out_layer = sess.run(model.out_layer, feed_dict={model.X:inputs})\n",
    "    return out_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vecs, sent_labels, lengths = train_gs.generate(32)\n",
    "out_layer = sent2vec(sent_vecs, sess_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_lm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_len = out_layer.shape[1]\n",
    "from sentiment_analysis import CNN_Model_Pretrained_Emb\n",
    "class trainConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = 200\n",
    "    hidden_size = 250\n",
    "    batch_size = 32\n",
    "    layer_size = 2\n",
    "    \n",
    "class testConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = 200\n",
    "    hidden_size = 250\n",
    "    batch_size = 32\n",
    "    layer_size = 2\n",
    "    \n",
    "class singleConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = 200\n",
    "    hidden_size = 250#hidden size for hidden state of rnn\n",
    "    batch_size = 1\n",
    "    layer_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "graph_cnn = tf.Graph()\n",
    "#Create models for training and testing data\n",
    "with graph_cnn.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.02, 0.02)\n",
    "    with tf.name_scope('train'):\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            train_model = CNN_Model_Pretrained_Emb(trainConfig)\n",
    "            saver_sent=tf.train.Saver()\n",
    "    with tf.name_scope('test'):\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            test_model = CNN_Model_Pretrained_Emb(testConfig, False)\n",
    "            single_model = CNN_Model_Pretrained_Emb(singleConfig, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunk_num  = int(len(text_train)/trainConfig.batch_size)\n",
    "test_chunk_num = int(len(text_test)/testConfig.batch_size)\n",
    "remain_num = len(text_test) - trainConfig.batch_size*test_chunk_num\n",
    "remain_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7179\n",
      "Loss: 0.5585\n",
      "Epoch 0 time:20.26\n",
      "Loss: 0.5651\n",
      "Loss: 0.6671\n",
      "Epoch 1 time:40.22\n"
     ]
    }
   ],
   "source": [
    "import time, os\n",
    "epochs = 2\n",
    "#train_chunk_num = 10\n",
    "file = \"ckpt_cnn_pretrained_emb/cnn.ckpt\"\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    #Initialize parameters\n",
    "    init = tf.global_variables_initializer()\n",
    "    if not os.path.exists(\"ckpt_cnn_pretrained_emb\"):\n",
    "        os.mkdir('ckpt_cnn_pretrained_emb')\n",
    "    if os.path.exists(\"ckpt_cnn_pretrained_emb/cnn.ckpt.index\"):\n",
    "        saver_sent.restore(sess, file)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    start_time = time.time()\n",
    "    for m in range(epochs):\n",
    "        for i in range(train_chunk_num):\n",
    "            #sess.run(tf.assign(learning_rate, 0.002*((0.98)**m)))\n",
    "            x, y, lengths = train_gs.generate(trainConfig.batch_size)\n",
    "            x = sent2vec(x, sess_lm)\n",
    "            feed_dict = {train_model.x:x, train_model.y:y, train_model.lengths:lengths}\n",
    "            l, _ = sess.run([train_model.cost, train_model.optimize], feed_dict=feed_dict)\n",
    "            if i%100 == 0:\n",
    "                print('Loss:', round(l, 4))\n",
    "        end_time = time.time()\n",
    "        print('Epoch', m, 'time:{:.2f}'.format(end_time - start_time))\n",
    "        \n",
    "    saver_sent.save(sess,'ckpt_cnn_pretrained_emb/cnn.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "INFO:tensorflow:Restoring parameters from ckpt_cnn_pretrained_emb/cnn.ckpt\n",
      "Parameters restored\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 20) for Tensor 'X:0', which has shape '(32, 20)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-1e20f75957a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#Traverse each data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_gs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess_lm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         feed_dict = {single_model.x:x, single_model.y:y, \n\u001b[1;32m     22\u001b[0m                      single_model.lengths:lengths}\n",
      "\u001b[0;32m<ipython-input-46-ec92d85c2b44>\u001b[0m in \u001b[0;36msent2vec\u001b[0;34m(inputs, sess)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#train_gs = generate_samples(np.array(text_train), np.array(y_train), word_to_idx, 20, False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#sent_vecs, sent_labels, lengths = train_gs.generate(32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mout_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1074\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1076\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1077\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (1, 20) for Tensor 'X:0', which has shape '(32, 20)'"
     ]
    }
   ],
   "source": [
    "#Calculate Testing Accuracy\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    print('Testing...')\n",
    "    count = 0\n",
    "    #saver = tf.train.import_meta_graph('ckpt_cnn/cnn.ckpt.meta')\n",
    "    saver_sent.restore(sess,tf.train.latest_checkpoint('ckpt_cnn_pretrained_emb/'))\n",
    "    print('Parameters restored')\n",
    "    start_time = time.time()\n",
    "    test_gs = generate_samples(np.array(text_test), np.array(y_test),word_to_idx, 20, False)\n",
    "    for _ in range(test_chunk_num):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_gs.generate(testConfig.batch_size)\n",
    "        x = sent2vec(x, sess_lm)\n",
    "        feed_dict = {test_model.x:x, test_model.y:y, test_model.lengths:lengths}\n",
    "        n = sess.run(test_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    for _ in range(remain_num):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_gs.generate(1)\n",
    "        x = sent2vec(x, sess_lm)\n",
    "        feed_dict = {single_model.x:x, single_model.y:y, \n",
    "                     single_model.lengths:lengths}\n",
    "        n = sess.run(single_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    end_time = time.time()\n",
    "    print('Testing Time:{:.2f}'.format(end_time - start_time))\n",
    "    print(count*1.0/len(test_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7308673469387755"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count*1.0/test_chunk_num/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "class generate_samples:\n",
    "    '''\n",
    "    Generate samples of training data or testing data for data analysis\n",
    "    '''\n",
    "    def __init__(self, data, labels, word_to_idx, max_sent_len=20, is_training=True):\n",
    "        '''\n",
    "        Args:\n",
    "        data: numpy\n",
    "        labels: numpy\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.is_training = is_training\n",
    "        self.max_sent_len = max_sent_len\n",
    "        self.index = 0\n",
    "        \n",
    "    def sent_split(self, sent):\n",
    "        '''\n",
    "        Split a sentence into tokens\n",
    "        '''\n",
    "        words = []\n",
    "        sent = nlp(sent.strip())\n",
    "        for w in sent:\n",
    "            words.append(w.text.lower())\n",
    "        return words\n",
    "        \n",
    "    def generate_samples(self, sents, labels, batch_size=64):\n",
    "        '''\n",
    "        Select a batch_size of sentences\n",
    "        Transform each sentence into a sequence of idx\n",
    "        '''\n",
    "        indice = np.random.choice(len(sents), batch_size)\n",
    "        sents = sents[indice]\n",
    "        labels = labels[indice]\n",
    "        #sent_vecs, sent_lens = self.create_sent_idx(sents)\n",
    "        sent_vecs, sent_lens = self.create_sent_idx(sents)\n",
    "        return sent_vecs, labels, sent_lens\n",
    "        #return self.create_sent_idx(sents), labels, sent_lens\n",
    "    \n",
    "    \n",
    "    def create_sent_idx(self, sents):\n",
    "        '''\n",
    "        Map sents into idx\n",
    "        '''\n",
    "        sents_lens = list(map(self.sent2idx, sents))\n",
    "        sents_idx, sents_lens = zip(*sents_lens)\n",
    "        return sents_idx, sents_lens\n",
    "        \n",
    "        \n",
    "    def sent2idx(self, sent):\n",
    "        '''Map a sentence into a sequence of idx'''\n",
    "        sent_idx = []\n",
    "        words = self.sent_split(str(sent))\n",
    "        lens = len(words)\n",
    "        ##Cut long sentences\n",
    "        if lens > self.max_sent_len:\n",
    "            words = words[:self.max_sent_len]\n",
    "            lens = self.max_sent_len\n",
    "        for w in words:\n",
    "            idx = self.word_to_idx.get(w)\n",
    "            idx = idx if idx else self.word_to_idx['<unk>']\n",
    "            sent_idx.append(idx)\n",
    "        ###Pad short sentences\n",
    "        for i in np.arange(lens, self.max_sent_len):\n",
    "            idx = self.word_to_idx['<pad>']\n",
    "            sent_idx.append(idx)\n",
    "        return sent_idx, lens\n",
    "    \n",
    "    def generate(self, batch_size=64):\n",
    "        if self.is_training:\n",
    "            sent_vecs, sent_labels, lengths = self.generate_samples(self.data, \n",
    "                                                               self.labels,\n",
    "                                                              batch_size)\n",
    "        else:\n",
    "            start = self.index\n",
    "            end = start + batch_size\n",
    "            if end > len(self.data):\n",
    "                print('Out of sample size')\n",
    "                self.index = 0\n",
    "            sents = self.data[start:end]\n",
    "            sent_labels = self.labels[start:end]\n",
    "            sent_vecs, lengths = self.create_sent_idx(sents)\n",
    "            self.index = end\n",
    "        return sent_vecs, sent_labels, lengths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
