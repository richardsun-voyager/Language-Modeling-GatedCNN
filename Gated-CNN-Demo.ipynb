{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep contextualized word representation has drawn wide attention because of state-of-the-art performances in downstream tasks. Contextualized embeddings can capture not only word-level information but also multi-sense information, thus improving the results in sentiment analysis, SQuad and etc. However, the language adopted in the [Elmo](https://allennlp.org/elmo) model were biLSTMs which contained a huge number of parameters, it was less likely for small labs to train and run such experiments.\n",
    "\n",
    "\n",
    "In this project, we intend to make use of CNN language model in learning efficient word representations for sentiment analysis. We train a language model based on [Gated CNN architecture](https://arxiv.org/abs/1612.08083) proposed by Yann Daulphin, then do sentiment analysis with embeddings generated by the language model.\n",
    "\n",
    "The language model training dataset is 1-billion-word-language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "\n",
    "from model import *\n",
    "from data_utils import data_helper\n",
    "from conf_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the configuration and prepare data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_size = 2000\n",
    "    embedding_size = 200\n",
    "    filter_size = 64\n",
    "    num_layers = 1\n",
    "    block_size = 2\n",
    "    filter_h = 5\n",
    "    context_size = 20\n",
    "    text_size = context_size\n",
    "    batch_size = 32\n",
    "    epochs = 5\n",
    "    num_sampled = 64\n",
    "    learning_rate = 0.0001\n",
    "    momentum = 0.99\n",
    "    grad_clip = 0.1\n",
    "    num_batches = 0\n",
    "    ckpt_path = 'ckpt'\n",
    "    summary_path = 'logs'\n",
    "    #data_dir = \"data/texts/reviews/movie_reviews\"\n",
    "    data_dir = \"data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize configuration files\n",
    "conf = prepare_conf(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data batches for language model training\n",
    "dh = data_helper(conf)\n",
    "x_batches, y_batches, word_to_idx, idx_to_word = dh.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.text_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_batches[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a CNN-based language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.text_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(640, 200), dtype=float32)\n",
      "Started Model Training...\n"
     ]
    }
   ],
   "source": [
    "#Create a language model\n",
    "#Note we need to save the models for subsequent tasks\n",
    "model = GatedCNN(conf)\n",
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "print(\"Started Model Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from RNN import RNN\n",
    "#model = RNN(conf)\n",
    "#saver = tf.train.Saver(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32895"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228.28345\n",
      "208.6025\n",
      "158.5471\n",
      "100.787155\n",
      "104.19407\n",
      "112.967186\n",
      "63.305267\n",
      "84.85893\n",
      "60.24416\n",
      "39.28103\n",
      "40.074116\n",
      "25.748016\n",
      "34.781715\n",
      "29.925396\n",
      "34.351105\n",
      "29.201511\n",
      "15.204874\n",
      "17.922197\n",
      "14.43259\n",
      "16.130964\n",
      "9.519644\n",
      "4.854629\n",
      "15.488127\n",
      "17.754185\n",
      "5.130473\n",
      "11.15888\n",
      "4.7705493\n",
      "4.6013713\n",
      "4.7350683\n",
      "4.91406\n",
      "6.793849\n",
      "4.506275\n",
      "4.7903934\n",
      "5.2425947\n",
      "6.6091423\n",
      "4.25373\n",
      "4.608274\n",
      "4.285217\n",
      "5.8390694\n",
      "4.3712454\n",
      "4.191039\n",
      "4.2742143\n",
      "4.2655087\n",
      "4.107478\n",
      "4.2508354\n",
      "4.176041\n",
      "4.422719\n",
      "4.129487\n",
      "4.24267\n",
      "4.236827\n",
      "4.114705\n",
      "4.1388817\n",
      "4.2875743\n",
      "4.259644\n",
      "4.15853\n",
      "4.2453675\n",
      "4.203578\n",
      "4.0906987\n",
      "4.2734327\n",
      "4.205821\n",
      "4.165262\n",
      "4.248075\n",
      "4.251493\n",
      "4.251256\n",
      "4.081624\n",
      "4.126757\n",
      "4.1385245\n",
      "4.069039\n",
      "4.212142\n",
      "4.1817803\n",
      "4.1815014\n",
      "4.183185\n",
      "4.236308\n",
      "4.125826\n",
      "4.1625557\n",
      "4.1035523\n",
      "4.16032\n",
      "4.057517\n",
      "4.1799\n",
      "4.1593404\n",
      "4.158732\n",
      "4.1533785\n",
      "4.0161977\n",
      "4.0019574\n",
      "4.221444\n",
      "4.0739126\n",
      "4.2144003\n",
      "4.1436186\n",
      "4.108012\n",
      "4.2313533\n",
      "4.1696095\n",
      "4.009533\n",
      "4.113341\n",
      "4.1602373\n",
      "4.1060157\n",
      "4.182256\n",
      "4.0892563\n",
      "3.9855568\n",
      "4.059971\n",
      "4.1261187\n",
      "4.133526\n",
      "4.176199\n",
      "4.12121\n",
      "4.0554338\n",
      "4.1574736\n",
      "4.031247\n",
      "4.008679\n",
      "4.0784993\n",
      "4.0442934\n",
      "4.175849\n",
      "4.0240474\n",
      "4.1919165\n",
      "3.9102738\n",
      "4.0912657\n",
      "4.1124997\n",
      "4.0467715\n",
      "4.1549582\n",
      "4.0245576\n",
      "4.022016\n",
      "4.059787\n",
      "3.9949603\n",
      "4.0812845\n",
      "4.163165\n",
      "4.0585694\n",
      "4.136506\n",
      "4.153099\n",
      "4.1247144\n",
      "3.9802012\n",
      "4.2172375\n",
      "4.1714263\n",
      "4.180941\n",
      "4.135492\n",
      "4.2218785\n",
      "4.173333\n",
      "4.1629295\n",
      "4.2230988\n",
      "4.1055827\n",
      "4.11818\n",
      "4.000907\n",
      "4.044839\n",
      "4.227763\n",
      "4.0560336\n",
      "4.097408\n",
      "4.24376\n",
      "4.0107317\n",
      "4.006227\n",
      "4.1995673\n",
      "4.098859\n",
      "4.2290454\n",
      "4.065398\n",
      "4.10501\n",
      "4.1106358\n",
      "4.164974\n",
      "3.9558856\n",
      "4.0535746\n",
      "4.044035\n",
      "4.162057\n",
      "4.196126\n",
      "3.903907\n",
      "4.0107703\n",
      "4.070983\n",
      "4.0605702\n",
      "4.1891584\n",
      "4.101786\n",
      "4.085089\n",
      "4.0817475\n",
      "3.940194\n",
      "3.9987044\n",
      "4.148665\n",
      "4.0421457\n",
      "4.0777464\n",
      "4.0822325\n",
      "4.0254674\n",
      "4.2207747\n",
      "4.0618567\n",
      "4.1031036\n",
      "4.137287\n",
      "4.0793824\n",
      "4.0473604\n",
      "4.048989\n",
      "4.170306\n",
      "4.079923\n",
      "4.004208\n",
      "4.061194\n",
      "4.063113\n",
      "4.081661\n",
      "4.0687017\n",
      "4.0366364\n",
      "4.144639\n",
      "4.03899\n",
      "3.9964042\n",
      "4.025102\n",
      "4.1156173\n",
      "4.018039\n",
      "4.0766687\n",
      "4.0202546\n",
      "4.0808983\n",
      "4.0043335\n",
      "4.2288084\n",
      "4.192445\n",
      "3.9203625\n",
      "4.1473503\n",
      "4.2334285\n",
      "4.0368586\n",
      "4.0562296\n",
      "4.1940284\n",
      "4.0686045\n",
      "4.0821147\n",
      "3.9223664\n",
      "4.1246843\n",
      "3.9316564\n",
      "4.1025796\n",
      "3.9700837\n",
      "4.0586104\n",
      "4.023179\n",
      "4.1793556\n",
      "4.070552\n",
      "4.0923157\n",
      "4.114414\n",
      "3.8970437\n",
      "4.0371737\n",
      "4.154981\n",
      "3.9737098\n",
      "4.1808715\n",
      "4.099541\n",
      "3.869658\n",
      "4.1038046\n",
      "4.081794\n",
      "4.026969\n",
      "3.9813647\n",
      "4.009595\n",
      "3.88454\n",
      "4.1992936\n",
      "4.155119\n",
      "4.0206556\n",
      "3.9510455\n",
      "4.0969787\n",
      "4.062108\n",
      "3.9839866\n",
      "4.0316715\n",
      "4.051401\n",
      "3.9904416\n",
      "4.027742\n",
      "4.1319075\n",
      "3.9828784\n",
      "3.960977\n",
      "4.038579\n",
      "4.0196605\n",
      "3.9955459\n",
      "4.0487833\n",
      "3.9787114\n",
      "3.9887547\n",
      "4.018402\n",
      "4.0881834\n",
      "4.075621\n",
      "4.0737133\n",
      "4.024932\n",
      "4.0690174\n",
      "4.0003214\n",
      "4.0249815\n",
      "4.1378107\n",
      "3.933288\n",
      "4.087564\n",
      "3.9551628\n",
      "4.2000513\n",
      "3.964887\n",
      "3.8941226\n",
      "4.027864\n",
      "4.0349226\n",
      "4.0993123\n",
      "4.0428057\n",
      "3.9365075\n",
      "3.9760613\n",
      "3.9477603\n",
      "3.9934235\n",
      "3.9474297\n",
      "3.848669\n",
      "4.118109\n",
      "3.9990647\n",
      "4.1676865\n",
      "4.1062875\n",
      "4.0251317\n",
      "4.0955253\n",
      "4.138442\n",
      "4.0580816\n",
      "3.9581094\n",
      "4.0040154\n",
      "4.0514135\n",
      "4.018098\n",
      "3.9143486\n",
      "4.028712\n",
      "3.989668\n",
      "4.117425\n",
      "3.9519546\n",
      "4.0937586\n",
      "3.9428515\n",
      "4.0270934\n",
      "4.039033\n",
      "4.024436\n",
      "4.0104628\n",
      "4.000644\n",
      "4.046563\n",
      "4.008178\n",
      "4.023135\n",
      "3.9423203\n",
      "4.0091906\n",
      "3.9980755\n",
      "3.839003\n",
      "4.1465673\n",
      "3.9487\n",
      "3.9994469\n",
      "4.1556983\n",
      "4.12246\n",
      "4.005248\n",
      "3.9358222\n",
      "4.0158606\n",
      "4.1022444\n",
      "3.9557033\n",
      "3.9732609\n",
      "4.009763\n",
      "3.8819976\n",
      "4.0206976\n",
      "3.9238079\n",
      "3.8954468\n",
      "3.9841952\n",
      "3.9209385\n",
      "4.042347\n",
      "3.9632869\n",
      "4.022901\n",
      "Epoch: 0.00, Time: 48.41,  Loss: 4.02\n",
      "Perplexity: 54.47\n",
      "3.9519677\n",
      "4.079564\n",
      "3.9935417\n",
      "4.0861335\n",
      "4.054383\n",
      "3.9276872\n",
      "3.9035923\n",
      "3.9359543\n",
      "4.048177\n",
      "4.1193113\n",
      "4.0628157\n",
      "4.064929\n",
      "4.0072603\n",
      "4.0732226\n",
      "3.9512386\n",
      "4.0021114\n",
      "3.9378772\n",
      "3.952275\n",
      "4.0712857\n",
      "4.0458193\n",
      "3.9687324\n",
      "3.968348\n",
      "3.9881344\n",
      "4.045544\n",
      "4.0849614\n",
      "3.9298835\n",
      "4.0119486\n",
      "3.9431503\n",
      "4.0041823\n",
      "4.0268736\n",
      "3.8939774\n",
      "4.1150594\n",
      "3.8677857\n",
      "3.9617512\n",
      "3.9358718\n",
      "4.018659\n",
      "3.9310257\n",
      "3.9450176\n",
      "4.1015644\n",
      "4.065207\n",
      "3.9633014\n",
      "4.0219398\n",
      "3.9190812\n",
      "3.911983\n",
      "4.0467725\n",
      "4.009511\n",
      "3.9892344\n",
      "3.9093716\n",
      "4.094895\n",
      "3.9550366\n",
      "3.8717163\n",
      "3.9509552\n",
      "4.0213194\n",
      "3.9778652\n",
      "3.8625417\n",
      "4.0913596\n",
      "3.9446754\n",
      "3.8920264\n",
      "4.071452\n",
      "3.9517295\n",
      "3.9505801\n",
      "4.0750914\n",
      "4.0356474\n",
      "3.9535937\n",
      "3.9551167\n",
      "3.9414773\n",
      "3.9369552\n",
      "3.8633552\n",
      "4.0256033\n",
      "4.0202084\n",
      "4.0446806\n",
      "3.9956117\n",
      "4.0367727\n",
      "4.073436\n",
      "4.037848\n",
      "3.9837594\n",
      "3.9776573\n",
      "3.864024\n",
      "4.0115137\n",
      "3.9160857\n",
      "3.923989\n",
      "3.940178\n",
      "3.8817723\n",
      "3.7690187\n",
      "4.030471\n",
      "3.861335\n",
      "3.9459603\n",
      "4.002345\n",
      "3.9326394\n",
      "4.054948\n",
      "4.059693\n",
      "3.8603046\n",
      "3.913163\n",
      "3.9053872\n",
      "3.983546\n",
      "3.9812622\n",
      "3.8704288\n",
      "3.8797545\n",
      "3.9081626\n",
      "3.9523683\n",
      "3.9442782\n",
      "3.99535\n",
      "3.9695568\n",
      "3.8774025\n",
      "3.9785385\n",
      "3.845531\n",
      "3.7771106\n",
      "3.892615\n",
      "3.9401963\n",
      "3.9025683\n",
      "3.9319515\n",
      "3.963739\n",
      "3.860765\n",
      "3.987231\n",
      "3.9166417\n",
      "3.9650753\n",
      "4.004411\n",
      "3.826912\n",
      "3.844295\n",
      "3.9412854\n",
      "3.7872257\n",
      "3.9531639\n",
      "3.975455\n",
      "3.8392181\n",
      "3.9808452\n",
      "4.052673\n",
      "4.0082364\n",
      "3.9081893\n",
      "4.0839534\n",
      "3.9718578\n",
      "4.0170383\n",
      "3.9117959\n",
      "3.9661727\n",
      "3.9751859\n",
      "3.9337993\n",
      "4.0590014\n",
      "3.881977\n",
      "3.966233\n",
      "3.8782806\n",
      "3.9126008\n",
      "3.9693322\n",
      "3.9403942\n",
      "3.9180214\n",
      "4.017127\n",
      "3.9157944\n",
      "3.8452935\n",
      "4.002965\n",
      "3.9716733\n",
      "4.150096\n",
      "3.8941312\n",
      "3.9645543\n",
      "3.932869\n",
      "4.018593\n",
      "3.8660264\n",
      "3.8478456\n",
      "3.8955817\n",
      "3.97677\n",
      "4.006711\n",
      "3.8787677\n",
      "3.8238647\n",
      "3.828762\n",
      "3.9055037\n",
      "4.046291\n",
      "3.849647\n",
      "3.9157398\n",
      "4.014114\n",
      "3.8640995\n",
      "3.8557944\n",
      "4.0480022\n",
      "3.9132469\n",
      "3.946527\n",
      "3.9415119\n",
      "3.8521454\n",
      "4.0899744\n",
      "3.980185\n",
      "3.874836\n",
      "3.9201252\n",
      "3.9961631\n",
      "3.9141564\n",
      "3.8555026\n",
      "3.922736\n",
      "3.910817\n",
      "3.9429374\n",
      "3.9378743\n",
      "3.9801095\n",
      "3.874639\n",
      "3.8711433\n",
      "3.9241645\n",
      "3.9585838\n",
      "3.9171307\n",
      "3.9236617\n",
      "3.862254\n",
      "4.001459\n",
      "3.8925183\n",
      "3.9192357\n",
      "3.8597665\n",
      "3.8245425\n",
      "3.7914028\n",
      "4.064107\n",
      "4.0374155\n",
      "3.785134\n",
      "3.9685807\n",
      "3.9433594\n",
      "4.0107346\n",
      "3.9026103\n",
      "3.9895859\n",
      "4.0228653\n",
      "3.828303\n",
      "3.8325279\n",
      "3.9916778\n",
      "3.8275352\n",
      "3.8996994\n",
      "3.7824395\n",
      "3.8771713\n",
      "3.9162838\n",
      "4.0293818\n",
      "3.9601707\n",
      "3.9540927\n",
      "3.9776115\n",
      "3.8318565\n",
      "3.8466516\n",
      "3.9635816\n",
      "3.9315796\n",
      "3.9922512\n",
      "4.0384417\n",
      "3.8575008\n",
      "3.899187\n",
      "3.9923215\n",
      "3.9790883\n",
      "3.9684372\n",
      "3.9717097\n",
      "3.8583493\n",
      "3.9817321\n",
      "3.9798703\n",
      "3.8307548\n",
      "3.8653336\n",
      "3.936414\n",
      "3.9729385\n",
      "3.8761902\n",
      "3.9033153\n",
      "3.9530036\n",
      "3.8845234\n",
      "3.9780204\n",
      "3.9097438\n",
      "3.8639169\n",
      "3.9214635\n",
      "3.9752498\n",
      "3.9150345\n",
      "3.864534\n",
      "3.9987667\n",
      "3.835497\n",
      "3.8863938\n",
      "3.9167705\n",
      "3.9257445\n",
      "3.9850883\n",
      "3.9449344\n",
      "3.9707687\n",
      "3.863818\n",
      "3.9343204\n",
      "3.9473233\n",
      "3.9476757\n",
      "3.8320885\n",
      "3.9524388\n",
      "3.920452\n",
      "4.0201616\n",
      "3.861829\n",
      "3.829604\n",
      "4.00671\n",
      "3.8538342\n",
      "3.8916192\n",
      "3.9040961\n",
      "3.865191\n",
      "3.873406\n",
      "3.880579\n",
      "3.8571823\n",
      "3.892404\n",
      "3.7891204\n",
      "3.9085011\n",
      "3.9957566\n",
      "3.9720893\n",
      "4.0193443\n",
      "3.912706\n",
      "3.9241576\n",
      "3.9596915\n",
      "3.8583503\n",
      "3.913945\n",
      "3.8943954\n",
      "4.0433335\n",
      "3.9507594\n",
      "3.8344002\n",
      "3.936016\n",
      "3.8301117\n",
      "3.9172618\n",
      "3.860009\n",
      "3.8988671\n",
      "3.8366547\n",
      "3.9425323\n",
      "3.8472805\n",
      "3.926513\n",
      "3.9660594\n",
      "3.9394298\n",
      "3.9718003\n",
      "3.887675\n",
      "3.9330094\n",
      "3.8896508\n",
      "3.7540736\n",
      "3.8632073\n",
      "3.7942257\n",
      "4.0241904\n",
      "3.8958888\n",
      "3.8664196\n",
      "3.936885\n",
      "4.0355864\n",
      "3.853516\n",
      "3.8672588\n",
      "3.927011\n",
      "3.939659\n",
      "3.823983\n",
      "3.8156624\n",
      "3.8483124\n",
      "3.8464859\n",
      "3.8945327\n",
      "3.8734276\n",
      "3.80376\n",
      "3.8584418\n",
      "3.8759048\n",
      "3.9210346\n",
      "3.8455224\n",
      "3.8557746\n",
      "Epoch: 1.00, Time: 47.31,  Loss: 3.85\n",
      "3.8241467\n",
      "4.0405235\n",
      "3.9036794\n",
      "3.984755\n",
      "3.908039\n",
      "3.8977196\n",
      "3.8919568\n",
      "3.855873\n",
      "3.8823369\n",
      "3.9419532\n",
      "3.9545784\n",
      "3.9285362\n",
      "3.880563\n",
      "3.9450843\n",
      "3.850428\n",
      "3.826011\n",
      "3.8230152\n",
      "3.9140732\n",
      "3.9338722\n",
      "3.9181232\n",
      "3.8639455\n",
      "3.8393848\n",
      "3.895843\n",
      "3.9497764\n",
      "3.899094\n",
      "3.8469493\n",
      "3.8572183\n",
      "3.8571963\n",
      "3.844747\n",
      "3.8976738\n",
      "3.77016\n",
      "3.9954\n",
      "3.8008933\n",
      "3.9485855\n",
      "3.8556447\n",
      "3.8671348\n",
      "3.8486283\n",
      "3.8707523\n",
      "3.8638484\n",
      "3.9344566\n",
      "3.833758\n",
      "3.9146893\n",
      "3.8114395\n",
      "3.7766266\n",
      "3.9081292\n",
      "3.8879075\n",
      "3.845623\n",
      "3.842809\n",
      "3.939397\n",
      "3.88937\n",
      "3.800569\n",
      "3.7556465\n",
      "3.9465249\n",
      "3.9271216\n",
      "3.8622112\n",
      "3.8839245\n",
      "3.8868225\n",
      "3.8396618\n",
      "3.9550548\n",
      "3.9324138\n",
      "3.8190887\n",
      "3.9973903\n",
      "3.8016472\n",
      "3.8746636\n",
      "3.826271\n",
      "3.8564026\n",
      "3.8705971\n",
      "3.725539\n",
      "3.87471\n",
      "3.9340966\n",
      "3.8998375\n",
      "3.7765229\n",
      "3.9282608\n",
      "3.9304783\n",
      "3.903191\n",
      "3.8701148\n",
      "3.8549907\n",
      "3.7568889\n",
      "3.8193812\n",
      "3.8293908\n",
      "3.8749447\n",
      "3.8914783\n",
      "3.8233178\n",
      "3.8083045\n",
      "3.9110837\n",
      "3.8357837\n",
      "3.9099612\n",
      "3.8915687\n",
      "3.8125184\n",
      "3.858825\n",
      "3.8897789\n",
      "3.822825\n",
      "3.816255\n",
      "3.8476436\n",
      "3.928307\n",
      "3.7875621\n",
      "3.8428874\n",
      "3.700028\n",
      "3.7694774\n",
      "3.827552\n",
      "3.841388\n",
      "3.8506348\n",
      "3.906901\n",
      "3.7855575\n",
      "3.879562\n",
      "3.760809\n",
      "3.7739913\n",
      "3.8711364\n",
      "3.8971667\n",
      "3.826201\n",
      "3.7478955\n",
      "3.846043\n",
      "3.7374523\n",
      "3.8202305\n",
      "3.8357518\n",
      "3.8427758\n",
      "3.8347538\n",
      "3.7458389\n",
      "3.6540368\n",
      "3.8258827\n",
      "3.745299\n",
      "3.8812404\n",
      "3.895623\n",
      "3.6858668\n",
      "3.8581035\n",
      "4.012641\n",
      "3.8673947\n",
      "3.7984881\n",
      "3.953282\n",
      "3.9560802\n",
      "3.9909387\n",
      "3.841056\n",
      "3.8295548\n",
      "3.8684032\n",
      "3.8515804\n",
      "3.9874916\n",
      "3.7714546\n",
      "3.837995\n",
      "3.802589\n",
      "3.8693619\n",
      "3.795704\n",
      "3.8484828\n",
      "3.867164\n",
      "3.9035811\n",
      "3.7904923\n",
      "3.805242\n",
      "3.8752208\n",
      "3.950978\n",
      "3.9456844\n",
      "3.8620896\n",
      "3.8426585\n",
      "3.915933\n",
      "3.972955\n",
      "3.7253633\n",
      "3.8436382\n",
      "3.85043\n",
      "3.929659\n",
      "3.8875365\n",
      "3.7734902\n",
      "3.8071716\n",
      "3.7402291\n",
      "3.7979407\n",
      "3.9057121\n",
      "3.8567758\n",
      "3.7830112\n",
      "3.8810477\n",
      "3.7861817\n",
      "3.845071\n",
      "3.9486191\n",
      "3.8446546\n",
      "3.8309314\n",
      "3.7539506\n",
      "3.8066978\n",
      "3.910561\n",
      "3.8416915\n",
      "3.8331757\n",
      "3.8825707\n",
      "3.8857434\n",
      "3.8272042\n",
      "3.8521087\n",
      "3.8949802\n",
      "3.8237405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8146527\n",
      "3.8352158\n",
      "3.8189282\n",
      "3.8980064\n",
      "3.7937012\n",
      "3.8909516\n",
      "3.8552842\n",
      "3.8255928\n",
      "3.8695092\n",
      "3.865809\n",
      "3.8576405\n",
      "3.8010106\n",
      "3.8385587\n",
      "3.7396502\n",
      "3.7994144\n",
      "3.7645729\n",
      "4.048583\n",
      "4.000402\n",
      "3.686047\n",
      "3.9688983\n",
      "3.9008183\n",
      "3.9297683\n",
      "3.7904804\n",
      "3.9080608\n",
      "3.859784\n",
      "3.760014\n",
      "3.7979932\n",
      "3.9005756\n",
      "3.742144\n",
      "3.9099407\n",
      "3.7749634\n",
      "3.8058465\n",
      "3.7907271\n",
      "3.9299178\n",
      "3.8839583\n",
      "3.8878617\n",
      "3.8886814\n",
      "3.7044272\n",
      "3.7893963\n",
      "3.9048645\n",
      "3.793591\n",
      "3.9308686\n",
      "3.9838052\n",
      "3.735489\n",
      "3.859272\n",
      "3.9235027\n",
      "3.8708458\n",
      "3.897118\n",
      "3.7819297\n",
      "3.7730942\n",
      "3.9242566\n",
      "3.914508\n",
      "3.791121\n",
      "3.7637558\n",
      "3.8108432\n",
      "3.9001842\n",
      "3.9082017\n",
      "3.8188434\n",
      "3.8070285\n",
      "3.788227\n",
      "3.9661782\n",
      "3.9224105\n",
      "3.7986386\n",
      "3.8654666\n",
      "3.9685674\n",
      "3.7913852\n",
      "3.781276\n",
      "3.8970933\n",
      "3.8334584\n",
      "3.7797897\n",
      "3.8435364\n",
      "3.8779728\n",
      "3.9667327\n",
      "3.9638188\n",
      "3.8470745\n",
      "3.8016975\n",
      "3.85028\n",
      "3.8261209\n",
      "3.9016156\n",
      "3.7042305\n",
      "3.8667111\n",
      "3.8518646\n",
      "3.9682987\n",
      "3.7704518\n",
      "3.8392262\n",
      "3.9365964\n",
      "3.8708825\n",
      "3.8713691\n",
      "3.8546124\n",
      "3.7848148\n",
      "3.8357296\n",
      "3.7532356\n",
      "3.8482394\n",
      "3.847215\n",
      "3.69712\n",
      "3.7927868\n",
      "3.9138057\n",
      "3.951427\n",
      "3.8880935\n",
      "3.9020653\n",
      "3.9058297\n",
      "3.8912673\n",
      "3.8298593\n",
      "3.8172467\n",
      "3.833634\n",
      "3.9280906\n",
      "3.8791847\n",
      "3.7665706\n",
      "3.8292003\n",
      "3.770232\n",
      "3.8185172\n",
      "3.8043156\n",
      "3.9362836\n",
      "3.7844338\n",
      "3.879417\n",
      "3.7763672\n",
      "3.8548648\n",
      "3.8705833\n",
      "3.8160465\n",
      "3.880447\n",
      "3.8028684\n",
      "3.8767154\n",
      "3.7891293\n",
      "3.8478248\n",
      "3.7808716\n",
      "3.77193\n",
      "3.9425836\n",
      "3.8552487\n",
      "3.8268962\n",
      "3.9498374\n",
      "3.9381752\n",
      "3.831101\n",
      "3.7862713\n",
      "3.8120785\n",
      "3.8503487\n",
      "3.7310805\n",
      "3.709954\n",
      "3.8399117\n",
      "3.718233\n",
      "3.7723312\n",
      "3.7842796\n",
      "3.7348278\n",
      "3.760406\n",
      "3.83339\n",
      "3.8135467\n",
      "3.6809235\n",
      "3.8318343\n",
      "Epoch: 2.00, Time: 48.43,  Loss: 3.85\n",
      "Perplexity: 45.55\n",
      "3.7700953\n",
      "3.9556775\n",
      "3.7772546\n",
      "3.9206555\n",
      "3.7499986\n",
      "3.7833877\n",
      "3.863702\n",
      "3.806722\n",
      "3.8161068\n",
      "3.927892\n",
      "3.883913\n",
      "3.89053\n",
      "3.8472729\n",
      "3.8772042\n",
      "3.7635093\n",
      "3.7919903\n",
      "3.7331786\n",
      "3.8121002\n",
      "3.953544\n",
      "3.8958926\n",
      "3.7537396\n",
      "3.8478458\n",
      "3.8117185\n",
      "3.8492026\n",
      "3.895874\n",
      "3.7035148\n",
      "3.8268178\n",
      "3.7391346\n",
      "3.8230948\n",
      "3.8395417\n",
      "3.760016\n",
      "3.8776352\n",
      "3.743297\n",
      "3.8182063\n",
      "3.8450618\n",
      "3.8710053\n",
      "3.8328903\n",
      "3.787197\n",
      "3.916798\n",
      "3.85957\n",
      "3.705785\n",
      "3.7566547\n",
      "3.7772954\n",
      "3.729707\n",
      "3.8966203\n",
      "3.8369648\n",
      "3.7952008\n",
      "3.7615151\n",
      "3.8298104\n",
      "3.8864791\n",
      "3.7535362\n",
      "3.650285\n",
      "3.9299445\n",
      "3.7845974\n",
      "3.7735515\n",
      "3.8491073\n",
      "3.8354087\n",
      "3.7420063\n",
      "3.8073297\n",
      "3.7662506\n",
      "3.7762542\n",
      "3.9389071\n",
      "3.854147\n",
      "3.7838013\n",
      "3.7688642\n",
      "3.8730493\n",
      "3.8058953\n",
      "3.779426\n",
      "3.792162\n",
      "3.8397155\n",
      "3.8917375\n",
      "3.7834115\n",
      "3.897353\n",
      "3.745795\n",
      "3.7881675\n",
      "3.841798\n",
      "3.7600574\n",
      "3.741324\n",
      "3.8788362\n",
      "3.817968\n",
      "3.8323684\n",
      "3.8305206\n",
      "3.719582\n",
      "3.707622\n",
      "3.8465092\n",
      "3.8445973\n",
      "3.798348\n",
      "3.841621\n",
      "3.7666156\n",
      "3.9148374\n",
      "3.7817771\n",
      "3.7221024\n",
      "3.7923417\n",
      "3.783247\n",
      "3.8720245\n",
      "3.7440734\n",
      "3.8391356\n",
      "3.7101517\n",
      "3.7539597\n",
      "3.7852955\n",
      "3.8269515\n",
      "3.8433998\n",
      "3.8800461\n",
      "3.7003586\n",
      "3.8185363\n",
      "3.652516\n",
      "3.7191036\n",
      "3.795697\n",
      "3.8209414\n",
      "3.7560363\n",
      "3.695438\n",
      "3.7794003\n",
      "3.661656\n",
      "3.8159668\n",
      "3.7238116\n",
      "3.8000531\n",
      "3.7822852\n",
      "3.6339364\n",
      "3.6213078\n",
      "3.7992845\n",
      "3.6835895\n",
      "3.7700984\n",
      "3.8455956\n",
      "3.7886643\n",
      "3.8269057\n",
      "3.900346\n",
      "3.8666809\n",
      "3.7454038\n",
      "3.9209545\n",
      "3.8849812\n",
      "3.8770719\n",
      "3.749237\n",
      "3.8469799\n",
      "3.8362784\n",
      "3.791946\n",
      "3.917101\n",
      "3.7503986\n",
      "3.8100257\n",
      "3.6860719\n",
      "3.815578\n",
      "3.7803035\n",
      "3.823468\n",
      "3.7729824\n",
      "3.8601327\n",
      "3.7612731\n",
      "3.7933586\n",
      "3.9447758\n",
      "3.8027413\n",
      "3.8314445\n",
      "3.7717156\n",
      "3.8156123\n",
      "3.8399196\n",
      "3.9251876\n",
      "3.683923\n",
      "3.7718635\n",
      "3.8051102\n",
      "3.7602851\n",
      "3.7660775\n",
      "3.7115395\n",
      "3.745147\n",
      "3.7728488\n",
      "3.7388623\n",
      "3.8795218\n",
      "3.7882164\n",
      "3.7594638\n",
      "3.8629174\n",
      "3.6875591\n",
      "3.7541783\n",
      "3.948523\n",
      "3.7936676\n",
      "3.7920868\n",
      "3.7653694\n",
      "3.7862396\n",
      "3.8493893\n",
      "3.788818\n",
      "3.7557054\n",
      "3.8259354\n",
      "3.8124118\n",
      "3.7903538\n",
      "3.7296631\n",
      "3.8228428\n",
      "3.677337\n",
      "3.7244244\n",
      "3.7928417\n",
      "3.86362\n",
      "3.89146\n",
      "3.7485745\n",
      "3.7095122\n",
      "3.780756\n",
      "3.7180355\n",
      "3.7301698\n",
      "3.7825825\n",
      "3.827256\n",
      "3.7444959\n",
      "3.7995465\n",
      "3.7163062\n",
      "3.7805455\n",
      "3.6554387\n",
      "3.9351826\n",
      "3.878976\n",
      "3.627613\n",
      "3.9126308\n",
      "3.9279506\n",
      "3.8167648\n",
      "3.7415817\n",
      "3.8482826\n",
      "3.7591624\n",
      "3.7183278\n",
      "3.665844\n",
      "3.8948758\n",
      "3.673479\n",
      "3.8619626\n",
      "3.7567115\n",
      "3.7756114\n",
      "3.7513523\n",
      "3.8752055\n",
      "3.7344642\n",
      "3.8047748\n",
      "3.8003669\n",
      "3.6284003\n",
      "3.732695\n",
      "3.8545508\n",
      "3.78655\n",
      "3.8557904\n",
      "3.8723614\n",
      "3.6992881\n",
      "3.6989505\n",
      "3.8575299\n",
      "3.7528396\n",
      "3.7689166\n",
      "3.752255\n",
      "3.747308\n",
      "3.8513825\n",
      "3.931583\n",
      "3.7212563\n",
      "3.7636535\n",
      "3.7839139\n",
      "3.8608787\n",
      "3.7726986\n",
      "3.7770317\n",
      "3.757564\n",
      "3.7297835\n",
      "3.8452656\n",
      "3.8095787\n",
      "3.7813237\n",
      "3.751609\n",
      "3.9409912\n",
      "3.7098877\n",
      "3.7343173\n",
      "3.7071826\n",
      "3.7841587\n",
      "3.7546966\n",
      "3.7531095\n",
      "3.803878\n",
      "3.9132714\n",
      "3.8853176\n",
      "3.7731557\n",
      "3.7385647\n",
      "3.7656379\n",
      "3.780032\n",
      "3.8410022\n",
      "3.7495422\n",
      "3.7258728\n",
      "3.8337102\n",
      "3.8838208\n",
      "3.710359\n",
      "3.7395859\n",
      "3.9068482\n",
      "3.740368\n",
      "3.7396915\n",
      "3.724608\n",
      "3.7569256\n",
      "3.7447715\n",
      "3.7016494\n",
      "3.7929168\n",
      "3.696209\n",
      "3.610227\n",
      "3.7787812\n",
      "3.8505387\n",
      "3.926453\n",
      "3.8826447\n",
      "3.8204837\n",
      "3.8831868\n",
      "3.8209572\n",
      "3.8032188\n",
      "3.7961922\n",
      "3.7895527\n",
      "3.8589497\n",
      "3.7655072\n",
      "3.664084\n",
      "3.7883115\n",
      "3.714431\n",
      "3.7694373\n",
      "3.7097905\n",
      "3.848724\n",
      "3.7633693\n",
      "3.8716903\n",
      "3.7996182\n",
      "3.8142276\n",
      "3.858071\n",
      "3.7577465\n",
      "3.854713\n",
      "3.799471\n",
      "3.8044667\n",
      "3.7419739\n",
      "3.7496388\n",
      "3.7438018\n",
      "3.7306817\n",
      "3.8885543\n",
      "3.8110893\n",
      "3.7913547\n",
      "3.8563638\n",
      "3.8970656\n",
      "3.6800942\n",
      "3.6210804\n",
      "3.7563515\n",
      "3.8164647\n",
      "3.6966386\n",
      "3.6885197\n",
      "3.7918649\n",
      "3.7107568\n",
      "3.7522984\n",
      "3.763127\n",
      "3.6775784\n",
      "3.7554677\n",
      "3.6511643\n",
      "3.794901\n",
      "3.705244\n",
      "3.7247157\n",
      "Epoch: 3.00, Time: 48.04,  Loss: 3.72\n",
      "3.7004647\n",
      "3.9288952\n",
      "3.7622554\n",
      "3.7692113\n",
      "3.720408\n",
      "3.8003166\n",
      "3.7613761\n",
      "3.774289\n",
      "3.8188019\n",
      "3.8940659\n",
      "3.8027916\n",
      "3.842558\n",
      "3.7660708\n",
      "3.8465657\n",
      "3.752095\n",
      "3.7839031\n",
      "3.677055\n",
      "3.77045\n",
      "3.8081245\n",
      "3.8021114\n",
      "3.7458382\n",
      "3.782888\n",
      "3.794796\n",
      "3.8508716\n",
      "3.7190356\n",
      "3.7510686\n",
      "3.7539356\n",
      "3.7911458\n",
      "3.7501884\n",
      "3.81508\n",
      "3.7621906\n",
      "3.8697345\n",
      "3.6493707\n",
      "3.7466977\n",
      "3.696277\n",
      "3.8363125\n",
      "3.8247967\n",
      "3.6925836\n",
      "3.8396144\n",
      "3.8347683\n",
      "3.757399\n",
      "3.8035336\n",
      "3.7000096\n",
      "3.657725\n",
      "3.8026931\n",
      "3.7797794\n",
      "3.733045\n",
      "3.7017322\n",
      "3.7589517\n",
      "3.7984746\n",
      "3.7028146\n",
      "3.6582425\n",
      "3.8181243\n",
      "3.8036888\n",
      "3.7608504\n",
      "3.8431942\n",
      "3.754399\n",
      "3.7050948\n",
      "3.7691295\n",
      "3.792986\n",
      "3.7150104\n",
      "3.8594222\n",
      "3.8015187\n",
      "3.732798\n",
      "3.7452781\n",
      "3.7886982\n",
      "3.7727096\n",
      "3.7162971\n",
      "3.8274102\n",
      "3.8515391\n",
      "3.8372447\n",
      "3.7019165\n",
      "3.8083434\n",
      "3.843719\n",
      "3.809668\n",
      "3.8272293\n",
      "3.7613938\n",
      "3.711898\n",
      "3.7186012\n",
      "3.7441769\n",
      "3.7744884\n",
      "3.8184688\n",
      "3.6380591\n",
      "3.7186387\n",
      "3.800885\n",
      "3.7454922\n",
      "3.7573612\n",
      "3.7802005\n",
      "3.712937\n",
      "3.8664017\n",
      "3.8033326\n",
      "3.6888509\n",
      "3.703629\n",
      "3.7945035\n",
      "3.7599366\n",
      "3.6724923\n",
      "3.742641\n",
      "3.6540284\n",
      "3.7122502\n",
      "3.725124\n",
      "3.7930458\n",
      "3.8006294\n",
      "3.7964718\n",
      "3.6193478\n",
      "3.7796798\n",
      "3.6231027\n",
      "3.6183057\n",
      "3.7622285\n",
      "3.7948785\n",
      "3.660455\n",
      "3.6563046\n",
      "3.7517579\n",
      "3.6327865\n",
      "3.742815\n",
      "3.709988\n",
      "3.6951587\n",
      "3.7999783\n",
      "3.5986438\n",
      "3.57742\n",
      "3.7056603\n",
      "3.5114746\n",
      "3.7405238\n",
      "3.8146358\n",
      "3.680357\n",
      "3.7929225\n",
      "3.828753\n",
      "3.789168\n",
      "3.685548\n",
      "3.9254766\n",
      "3.8318703\n",
      "3.8434556\n",
      "3.7355285\n",
      "3.7659962\n",
      "3.7379098\n",
      "3.7896638\n",
      "3.8919163\n",
      "3.698813\n",
      "3.77977\n",
      "3.727105\n",
      "3.7541606\n",
      "3.7065125\n",
      "3.7258103\n",
      "3.7320552\n",
      "3.9113421\n",
      "3.6877034\n",
      "3.6098275\n",
      "3.850575\n",
      "3.8018563\n",
      "3.8247437\n",
      "3.6955318\n",
      "3.723338\n",
      "3.80652\n",
      "3.847384\n",
      "3.6550465\n",
      "3.7429547\n",
      "3.686483\n",
      "3.8229485\n",
      "3.7972527\n",
      "3.658276\n",
      "3.6938522\n",
      "3.6614258\n",
      "3.7371688\n",
      "3.7662594\n",
      "3.6316319\n",
      "3.7494664\n",
      "3.8096175\n",
      "3.697715\n",
      "3.7124572\n",
      "3.8129158\n",
      "3.7151916\n",
      "3.749847\n",
      "3.725678\n",
      "3.6850762\n",
      "3.7669213\n",
      "3.6694787\n",
      "3.6986783\n",
      "3.7347538\n",
      "3.7863889\n",
      "3.7430527\n",
      "3.7393956\n",
      "3.8009982\n",
      "3.711949\n",
      "3.6928592\n",
      "3.8182635\n",
      "3.747768\n",
      "3.7901664\n",
      "3.7298875\n",
      "3.7463317\n",
      "3.7687192\n",
      "3.741815\n",
      "3.581031\n",
      "3.6785424\n",
      "3.7582924\n",
      "3.703144\n",
      "3.7759373\n",
      "3.760112\n",
      "3.688458\n",
      "3.5954194\n",
      "3.9309502\n",
      "3.8609593\n",
      "3.6130378\n",
      "3.8846612\n",
      "3.8694623\n",
      "3.804528\n",
      "3.6310353\n",
      "3.843808\n",
      "3.8030312\n",
      "3.7314868\n",
      "3.6855502\n",
      "3.748553\n",
      "3.599418\n",
      "3.7785408\n",
      "3.7033086\n",
      "3.7393475\n",
      "3.6840255\n",
      "3.74303\n",
      "3.7684522\n",
      "3.8451412\n",
      "3.7278106\n",
      "3.6220307\n",
      "3.726193\n",
      "3.7747436\n",
      "3.7495117\n",
      "3.830117\n",
      "3.7517447\n",
      "3.6315017\n",
      "3.6833827\n",
      "3.7750359\n",
      "3.6982865\n",
      "3.7756743\n",
      "3.6588204\n",
      "3.7044845\n",
      "3.8384082\n",
      "3.8856168\n",
      "3.7162273\n",
      "3.6904616\n",
      "3.7883606\n",
      "3.860698\n",
      "3.8006814\n",
      "3.7540298\n",
      "3.6904132\n",
      "3.7409198\n",
      "3.8575292\n",
      "3.7747664\n",
      "3.7254722\n",
      "3.745449\n",
      "3.8955605\n",
      "3.6762059\n",
      "3.6422157\n",
      "3.80774\n",
      "3.7297733\n",
      "3.7016816\n",
      "3.6907895\n",
      "3.7480826\n",
      "3.7426472\n",
      "3.851626\n",
      "3.6884422\n",
      "3.576301\n",
      "3.7196193\n",
      "3.7580009\n",
      "3.856701\n",
      "3.655188\n",
      "3.7641723\n",
      "3.7370555\n",
      "3.8430989\n",
      "3.6939201\n",
      "3.6389332\n",
      "3.8557982\n",
      "3.7131047\n",
      "3.8073947\n",
      "3.7202823\n",
      "3.6919627\n",
      "3.6720033\n",
      "3.6680362\n",
      "3.6549811\n",
      "3.7133431\n",
      "3.6110203\n",
      "3.7024665\n",
      "3.8110347\n",
      "3.8828921\n",
      "3.7533221\n",
      "3.7430801\n",
      "3.8142776\n",
      "3.8186393\n",
      "3.7373092\n",
      "3.7168167\n",
      "3.704625\n",
      "3.822681\n",
      "3.7245362\n",
      "3.6413956\n",
      "3.7129395\n",
      "3.6789355\n",
      "3.7337132\n",
      "3.7219467\n",
      "3.8019905\n",
      "3.73511\n",
      "3.808298\n",
      "3.763657\n",
      "3.8158937\n",
      "3.7485719\n",
      "3.7632709\n",
      "3.7483552\n",
      "3.6872845\n",
      "3.8040967\n",
      "3.7528598\n",
      "3.691484\n",
      "3.71426\n",
      "3.7351556\n",
      "3.8710716\n",
      "3.7864482\n",
      "3.7601275\n",
      "3.8289375\n",
      "3.8909707\n",
      "3.6857305\n",
      "3.6334007\n",
      "3.7224076\n",
      "3.862163\n",
      "3.6402347\n",
      "3.6223226\n",
      "3.6733913\n",
      "3.6102405\n",
      "3.688675\n",
      "3.6741982\n",
      "3.6028066\n",
      "3.7026634\n",
      "3.7496865\n",
      "3.801204\n",
      "3.6810222\n",
      "3.7169945\n",
      "Epoch: 4.00, Time: 49.07,  Loss: 3.78\n",
      "Perplexity: 42.11\n"
     ]
    }
   ],
   "source": [
    "batch_idx = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter(conf.summary_path, graph=sess.graph)\n",
    "\n",
    "    #if os.path.exists(conf.ckpt_file):\n",
    "        #saver.restore(sess, conf.ckpt_file)\n",
    "        #print(\"Model Restored\")\n",
    "\n",
    "    for i in np.arange(conf.epochs):\n",
    "        start = time.time()\n",
    "        for j in np.arange(conf.num_batches):\n",
    "        #for j in np.arange(21):\n",
    "            inputs, labels, batch_idx = dh.get_batch(x_batches, y_batches, batch_idx)\n",
    "            _, l = sess.run([model.optimizer, model.loss], feed_dict={model.X:inputs, model.y:labels})\n",
    "            if j%100 == 0:\n",
    "                print(l)\n",
    "        end = time.time()\n",
    "        print(\"Epoch: %.2f, Time: %.2f,  Loss: %.2f\"%(i, end-start, l))\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            perp = sess.run(model.perplexity, feed_dict={model.X:inputs, model.y:labels})\n",
    "            print(\"Perplexity: %.2f\"%perp)\n",
    "            saver.save(sess, conf.ckpt_file)\n",
    "\n",
    "        summaries = sess.run(model.merged_summary_op, feed_dict={model.X:inputs, model.y:labels})\n",
    "        summary_writer.add_summary(summaries, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "In this part, we need to use other datasets for sentiment analysis, like the one of SemEval2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use spacy tokenize sentences into words\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en\n",
    "#import spacy\n",
    "#nlp = spacy.load('en')\n",
    "#def sent_split(sent):\n",
    "    #words = []\n",
    "    #sent = nlp(sent.strip())\n",
    "    #for w in sent:\n",
    "        #words.append(w.text.lower())\n",
    "    #return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = 'data/semeval/downloaded.tsv'\n",
    "file_dev = 'data/semeval/dev_downloaded.tsv'\n",
    "file_test = 'data/semeval/test.txt'\n",
    "with open(file_train) as f:\n",
    "    tweets_train = f.readlines()\n",
    "with open(file_dev) as f:\n",
    "    tweets_dev = f.readlines()\n",
    "with open(file_test) as f:\n",
    "    tweets_test = f.readlines()\n",
    "    \n",
    "\n",
    "\n",
    "#Filter empty tweets\n",
    "def is_available(text):\n",
    "    if 'Not Available' in text:\n",
    "        return False\n",
    "    if '\\t\"objective' in text:\n",
    "        return False\n",
    "    if '\\t\"neutral' in text:\n",
    "        return False\n",
    "    if '\\tobjective' in text:\n",
    "        return False\n",
    "    if '\\tneutral' in text:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = list(filter(is_available, tweets_train))\n",
    "tweets_dev = list(filter(is_available, tweets_dev))\n",
    "tweets_test = list(filter(is_available, tweets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = [item.split('\\t') for item in tweets_train]\n",
    "tweets_dev = [item.split('\\t') for item in tweets_dev]\n",
    "tweets_test = [item.split('\\t') for item in tweets_test]\n",
    "_, _, y_train, text_train = list(zip(*tweets_train))\n",
    "_, _, y_dev, text_dev = list(zip(*tweets_dev))\n",
    "_, _, y_test, text_test = list(zip(*tweets_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, y_train = list(text_train), list(y_train)\n",
    "text_dev, y_dev = list(text_dev), list(y_dev)\n",
    "text_test, y_test = list(text_test), list(y_test)\n",
    "y_test = ['\"' + item + '\"' for item in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the labels to numbers\n",
    "from sklearn import preprocessing\n",
    "label_encode = preprocessing.LabelEncoder()  # 建立模型\n",
    "y_train = label_encode.fit_transform(y_train)\n",
    "y_dev = label_encode.transform(y_dev)\n",
    "y_test = label_encode.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "class generate_samples:\n",
    "    '''\n",
    "    Generate samples of training data or testing data for data analysis\n",
    "    '''\n",
    "    def __init__(self, data, labels, word_to_idx, max_sent_len=20, is_training=True):\n",
    "        '''\n",
    "        Args:\n",
    "        data: numpy\n",
    "        labels: numpy\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.is_training = is_training\n",
    "        self.max_sent_len = max_sent_len\n",
    "        self.index = 0\n",
    "        \n",
    "    def sent_split(self, sent):\n",
    "        '''\n",
    "        Split a sentence into tokens\n",
    "        '''\n",
    "        words = []\n",
    "        sent = nlp(sent.strip())\n",
    "        for w in sent:\n",
    "            words.append(w.text.lower())\n",
    "        return words\n",
    "        \n",
    "    def generate_samples(self, sents, labels, batch_size=64):\n",
    "        '''\n",
    "        Select a batch_size of sentences\n",
    "        Transform each sentence into a sequence of idx\n",
    "        '''\n",
    "        indice = np.random.choice(len(sents), batch_size)\n",
    "        sents = sents[indice]\n",
    "        labels = labels[indice]\n",
    "        #sent_vecs, sent_lens = self.create_sent_idx(sents)\n",
    "        sent_vecs, sent_lens = self.create_sent_idx(sents)\n",
    "        return sent_vecs, labels, sent_lens\n",
    "        #return self.create_sent_idx(sents), labels, sent_lens\n",
    "    \n",
    "    \n",
    "    def create_sent_idx(self, sents):\n",
    "        '''\n",
    "        Map sents into idx\n",
    "        '''\n",
    "        sents_lens = list(map(self.sent2idx, sents))\n",
    "        sents_idx, sents_lens = zip(*sents_lens)\n",
    "        return sents_idx, sents_lens\n",
    "        \n",
    "        \n",
    "    def sent2idx(self, sent):\n",
    "        '''Map a sentence into a sequence of idx'''\n",
    "        sent_idx = []\n",
    "        words = self.sent_split(str(sent))\n",
    "        lens = len(words)\n",
    "        ##Cut long sentences\n",
    "        if lens > self.max_sent_len:\n",
    "            words = words[:self.max_sent_len]\n",
    "            lens = self.max_sent_len\n",
    "        for w in words:\n",
    "            idx = self.word_to_idx.get(w)\n",
    "            idx = idx if idx else self.word_to_idx['<unk>']\n",
    "            sent_idx.append(idx)\n",
    "        ###Pad short sentences\n",
    "        for i in np.arange(lens, self.max_sent_len):\n",
    "            idx = self.word_to_idx['<pad>']\n",
    "            sent_idx.append(idx)\n",
    "        return sent_idx, lens\n",
    "    \n",
    "    def generate(self, batch_size=64):\n",
    "        if self.is_training:\n",
    "            sent_vecs, sent_labels, lengths = self.generate_samples(self.data, \n",
    "                                                               self.labels,\n",
    "                                                              batch_size)\n",
    "        else:\n",
    "            start = self.index\n",
    "            end = start + batch_size\n",
    "            if end > len(self.data):\n",
    "                print('Out of sample size')\n",
    "                self.index = 0\n",
    "            sents = self.data[start:end]\n",
    "            sent_labels = self.labels[start:end]\n",
    "            sent_vecs, lengths = self.create_sent_idx(sents)\n",
    "            self.index = end\n",
    "        return sent_vecs, sent_labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sents_handler import generate_samples\n",
    "import numpy as np\n",
    "\n",
    "train_gs = generate_samples(np.array(text_train), np.array(y_train), word_to_idx, 20, False)\n",
    "#sent_vecs, sent_labels, lengths = train_gs.generate(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/vocab2000_embed200_filters64_batch32_layers1_block2_fdim5/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, conf.ckpt_file)\n",
    "    #Get the contextualized representation\n",
    "    #train_gs = generate_samples(np.array(text_train), np.array(y_train), word_to_idx, 20, False)\n",
    "    sent_vecs, sent_labels, lengths = train_gs.generate(32)\n",
    "    out_layer = sess.run(model.out_layer, feed_dict={model.X:sent_vecs})\n",
    "    \n",
    "    out_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_len = out_layer.shape[1]\n",
    "from sentiment_analysis import CNN_Model_Pretrained_Emb\n",
    "class trainConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = 200\n",
    "    hidden_size = 250\n",
    "    batch_size = 64\n",
    "    layer_size = 2\n",
    "    \n",
    "class testConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = 200\n",
    "    hidden_size = 250\n",
    "    batch_size = 64\n",
    "    layer_size = 2\n",
    "    \n",
    "class singleConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = 200\n",
    "    hidden_size = 250#hidden size for hidden state of rnn\n",
    "    batch_size = 1\n",
    "    layer_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "graph_cnn = tf.Graph()\n",
    "#Create models for training and testing data\n",
    "with graph_cnn.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.02, 0.02)\n",
    "    with tf.name_scope('train'):\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            train_model = CNN_Model_Pretrained_Emb(trainConfig)\n",
    "            saver=tf.train.Saver()\n",
    "    with tf.name_scope('test'):\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            test_model = CNN_Model_Pretrained_Emb(testConfig, False)\n",
    "            single_model = CNN_Model_Pretrained_Emb(singleConfig, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "epochs = 2\n",
    "#train_chunk_num = 10\n",
    "file = \"ckpt_cnn_pretrained_emb/cnn.ckpt\"\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    #Initialize parameters\n",
    "    init = tf.global_variables_initializer()\n",
    "    if not os.path.exists(\"ckpt_cnn_pretrained_emb\"):\n",
    "        os.mkdir('ckpt_cnn_pretrained_emb')\n",
    "    if os.path.exists(\"ckpt_cnn_pretrained_emb/cnn.ckpt.index\"):\n",
    "        saver.restore(sess, file)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    start_time = time.time()\n",
    "    for m in range(epochs):\n",
    "        for i in range(train_chunk_num):\n",
    "            #sess.run(tf.assign(learning_rate, 0.002*((0.98)**m)))\n",
    "            x, y, lengths = train_gs.generate(trainConfig.batch_size)\n",
    "            feed_dict = {train_model.x:x, train_model.y:y, train_model.lengths:lengths}\n",
    "            l, _ = sess.run([train_model.cost, train_model.optimize], feed_dict=feed_dict)\n",
    "            if i%100 == 0:\n",
    "                print('Loss:', round(l, 4))\n",
    "        end_time = time.time()\n",
    "        print('Epoch', m, 'time:{:.2f}'.format(end_time - start_time))\n",
    "        \n",
    "    saver.save(sess,'ckpt_cnn_pretrained_emb/cnn.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Testing Accuracy\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    print('Testing...')\n",
    "    count = 0\n",
    "    #saver = tf.train.import_meta_graph('ckpt_cnn/cnn.ckpt.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('ckpt_cnn_pretrained_emb/'))\n",
    "    print('Parameters restored')\n",
    "    start_time = time.time()\n",
    "    test_gs = generate_samples(np.array(test_processed), np.array(test_labels), False)\n",
    "    for _ in range(test_chunk_num):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_gs.generate(testConfig.batch_size)\n",
    "        feed_dict = {test_model.x:x, test_model.y:y, test_model.lengths:lengths}\n",
    "        n = sess.run(test_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    for _ in range(remain_num):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_gs.generate(1)\n",
    "        feed_dict = {single_model.x:x, single_model.y:y, \n",
    "                     single_model.lengths:lengths}\n",
    "        n = sess.run(single_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    end_time = time.time()\n",
    "    print('Testing Time:{:.2f}'.format(end_time - start_time))\n",
    "    print(count*1.0/len(test_processed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
