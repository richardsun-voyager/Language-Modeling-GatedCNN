{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep contextualized word representation has drawn wide attention because of state-of-the-art performances in downstream tasks. Contextualized embeddings can capture not only word-level information but also multi-sense information, thus improving the results in sentiment analysis, SQuad and etc. However, the language adopted in the [Elmo](https://allennlp.org/elmo) model were biLSTMs which contained a huge number of parameters, it was less likely for small labs to train and run such experiments.\n",
    "\n",
    "\n",
    "In this project, we intend to make use of CNN language model in learning efficient word representations for sentiment analysis. We train a language model based on [Gated CNN architecture](https://arxiv.org/abs/1612.08083) proposed by Yann Daulphin, then do sentiment analysis with embeddings generated by the language model.\n",
    "\n",
    "The language model training dataset is 1-billion-word-language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from model import GatedCharCNN\n",
    "from bilm.training import load_options_latest_checkpoint, load_vocab\n",
    "from bilm.data import Batcher, BidirectionalLMDataset\n",
    "from data_utils import data_helper\n",
    "from conf_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the configuration and prepare data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the words\n",
    "vocab_file = 'data/vocab-2016-09-10.txt'\n",
    "vocab = load_vocab(vocab_file, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_size = vocab.size\n",
    "    embedding_size = 100\n",
    "    filter_size = 64\n",
    "    num_layers = 3\n",
    "    block_size = 3\n",
    "    filter_h = 5\n",
    "    context_size = 20\n",
    "    text_size = context_size\n",
    "    batch_size = 32\n",
    "    epochs = 5\n",
    "    num_sampled = 64\n",
    "    learning_rate = 0.0001\n",
    "    momentum = 0.99\n",
    "    grad_clip = 0.1\n",
    "    num_batches = 0\n",
    "    ckpt_path = 'ckpt'\n",
    "    summary_path = 'logs'\n",
    "    #data_dir = \"data/texts/reviews/movie_reviews\"\n",
    "    data_dir = \"data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize configuration files\n",
    "conf = prepare_conf(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prefix = 'data/1-billion-word-language-modeling-ben\\\n",
    "chmark-r13output/training-monolingual.tokenized.shuffled/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 99 shards at data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00017-of-00100\n",
      "Loaded 306553 sentences.\n",
      "Finished loading\n",
      "Found 99 shards at data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00043-of-00100\n",
      "Loaded 306300 sentences.\n",
      "Finished loading\n"
     ]
    }
   ],
   "source": [
    "data = BidirectionalLMDataset(train_prefix, vocab, test=False,\n",
    "                                      shuffle_on_load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = data.iter_batches(conf.batch_size * 1, conf.text_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 20, 50)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['tokens_characters'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 20)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['next_token_id'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token_ids', 'tokens_characters', 'next_token_id', 'token_ids_reverse', 'tokens_characters_reverse', 'next_token_id_reverse'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a CharCNN-based language model\n",
    "\n",
    "Note the inputs are transformed into chars of words, so as to make use of subword information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Model Training...\n"
     ]
    }
   ],
   "source": [
    "#Create a language model\n",
    "#Note we need to save the models for subsequent tasks\n",
    "model = GatedCharCNN(conf)\n",
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "print(\"Started Model Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from RNN import RNN\n",
    "#model = RNN(conf)\n",
    "#saver = tf.train.Saver(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_3:0' shape=(?, 1) dtype=int32>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/vocab793471_embed100_filters64_batch32_layers3_block3_fdim5/model.ckpt\n",
      "Model Restored\n",
      "5.871673\n",
      "6.0465837\n",
      "5.8340435\n",
      "5.518068\n",
      "5.5001335\n",
      "5.5706367\n",
      "5.6483393\n",
      "5.867879\n",
      "5.69308\n",
      "5.578862\n",
      "5.6097913\n",
      "5.8520594\n",
      "5.561614\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00002-of-00100\n",
      "Loaded 307000 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00015-of-00100\n",
      "Loaded 306329 sentences.\n",
      "Finished loading\n",
      "5.474095\n",
      "5.7163305\n",
      "5.467099\n",
      "5.498586\n",
      "5.5851455\n",
      "5.5239353\n",
      "5.6595364\n",
      "Epoch: 0.00, Time: 6085.33,  Loss: 5.53\n",
      "Perplexity: 250.83\n",
      "5.525958\n",
      "5.678918\n",
      "5.764942\n",
      "5.5059214\n",
      "5.5593147\n",
      "5.5702057\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00090-of-00100\n",
      "Loaded 306997 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00077-of-00100\n",
      "Loaded 305798 sentences.\n",
      "Finished loading\n",
      "5.4001284\n",
      "5.4081426\n",
      "5.3473616\n",
      "5.673064\n",
      "5.463938\n",
      "5.4806013\n",
      "5.6236086\n",
      "5.552393\n",
      "5.4018683\n",
      "5.6002216\n",
      "5.3368106\n",
      "5.7085924\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00050-of-00100\n",
      "Loaded 305220 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00025-of-00100\n",
      "Loaded 306273 sentences.\n",
      "Finished loading\n",
      "5.544526\n",
      "5.366388\n",
      "Epoch: 1.00, Time: 6185.64,  Loss: 5.67\n",
      "5.7042136\n",
      "5.506734\n",
      "5.2962046\n",
      "5.206308\n",
      "5.340075\n",
      "5.6786194\n",
      "5.3260107\n",
      "5.5388193\n",
      "5.387238\n",
      "5.477057\n",
      "5.5363083\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00057-of-00100\n",
      "Loaded 305084 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00019-of-00100\n",
      "Loaded 305591 sentences.\n",
      "Finished loading\n",
      "5.128054\n",
      "5.4591036\n",
      "5.3717117\n",
      "5.615518\n",
      "5.4941244\n",
      "5.5772996\n",
      "5.32123\n",
      "5.5527053\n",
      "5.6013174\n",
      "Epoch: 2.00, Time: 6194.48,  Loss: 5.30\n",
      "Perplexity: 199.26\n",
      "5.3560286\n",
      "5.465136\n",
      "5.302752\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00003-of-00100\n",
      "Loaded 305915 sentences.\n",
      "Finished loading\n",
      "5.24573\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00052-of-00100\n",
      "Loaded 305378 sentences.\n",
      "Finished loading\n",
      "5.619803\n",
      "5.424063\n",
      "5.2020383\n",
      "5.361752\n",
      "5.6225653\n",
      "5.354541\n",
      "5.3715873\n",
      "5.385377\n",
      "5.5216193\n",
      "5.5360794\n",
      "5.4685745\n",
      "5.362756\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00046-of-00100\n",
      "Loaded 305308 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00061-of-00100\n",
      "Loaded 306420 sentences.\n",
      "Finished loading\n",
      "5.528595\n",
      "5.3401823\n",
      "5.383372\n",
      "5.618306\n",
      "Epoch: 3.00, Time: 6255.04,  Loss: 5.33\n",
      "5.4640303\n",
      "5.2185683\n",
      "5.4142356\n",
      "5.243071\n",
      "5.169265\n",
      "5.355648\n",
      "5.294385\n",
      "5.2995367\n",
      "5.6729956\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00055-of-00100\n",
      "Loaded 306641 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00073-of-00100\n",
      "Loaded 306690 sentences.\n",
      "Finished loading\n",
      "5.435672\n",
      "5.196258\n",
      "5.4403534\n",
      "5.3833055\n",
      "5.5194664\n",
      "5.234497\n",
      "5.627652\n",
      "5.2531176\n",
      "5.418557\n",
      "5.4442825\n",
      "5.4241133\n",
      "Epoch: 4.00, Time: 6231.40,  Loss: 5.40\n",
      "Perplexity: 220.95\n"
     ]
    }
   ],
   "source": [
    "batch_idx = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter(conf.summary_path, graph=sess.graph)\n",
    "\n",
    "    if os.path.exists(conf.ckpt_file+'.index'):\n",
    "        saver.restore(sess, conf.ckpt_file)\n",
    "        print(\"Model Restored\")\n",
    "\n",
    "    for i in np.arange(conf.epochs):\n",
    "        start = time.time()\n",
    "        for j in np.arange(20000):\n",
    "        #for j in np.arange(21):\n",
    "            x = next(data_gen)\n",
    "            inputs, labels = x['tokens_characters'], x['next_token_id']\n",
    "            labels = labels.reshape(-1, 1)\n",
    "            _, l = sess.run([model.optimizer, model.loss], feed_dict={model.X:inputs, model.y:labels})\n",
    "            if j%1000 == 0:\n",
    "                print(l)\n",
    "        end = time.time()\n",
    "        print(\"Epoch: %.2f, Time: %.2f,  Loss: %.2f\"%(i, end-start, l))\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            perp = sess.run(model.perplexity, feed_dict={model.X:inputs, model.y:labels})\n",
    "            print(\"Perplexity: %.2f\"%perp)\n",
    "            saver.save(sess, conf.ckpt_file)\n",
    "\n",
    "        summaries = sess.run(model.merged_summary_op, feed_dict={model.X:inputs, model.y:labels})\n",
    "        summary_writer.add_summary(summaries, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "In this part, we need to use other datasets for sentiment analysis, like the one of SemEval2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = 'data/semeval/downloaded.tsv'\n",
    "file_dev = 'data/semeval/dev_downloaded.tsv'\n",
    "file_test = 'data/semeval/test.txt'\n",
    "with open(file_train) as f:\n",
    "    tweets_train = f.readlines()\n",
    "with open(file_dev) as f:\n",
    "    tweets_dev = f.readlines()\n",
    "with open(file_test) as f:\n",
    "    tweets_test = f.readlines()\n",
    "    \n",
    "\n",
    "\n",
    "#Filter empty tweets\n",
    "def is_available(text):\n",
    "    if 'Not Available' in text:\n",
    "        return False\n",
    "    if '\\t\"objective' in text:\n",
    "        return False\n",
    "    if '\\t\"neutral' in text:\n",
    "        return False\n",
    "    if '\\tobjective' in text:\n",
    "        return False\n",
    "    if '\\tneutral' in text:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = list(filter(is_available, tweets_train))\n",
    "tweets_dev = list(filter(is_available, tweets_dev))\n",
    "tweets_test = list(filter(is_available, tweets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = [item.split('\\t') for item in tweets_train]\n",
    "tweets_dev = [item.split('\\t') for item in tweets_dev]\n",
    "tweets_test = [item.split('\\t') for item in tweets_test]\n",
    "_, _, y_train, text_train = list(zip(*tweets_train))\n",
    "_, _, y_dev, text_dev = list(zip(*tweets_dev))\n",
    "_, _, y_test, text_test = list(zip(*tweets_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, y_train = list(text_train), list(y_train)\n",
    "text_dev, y_dev = list(text_dev), list(y_dev)\n",
    "text_test, y_test = list(text_test), list(y_test)\n",
    "y_test = ['\"' + item + '\"' for item in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the labels to numbers\n",
    "from sklearn import preprocessing\n",
    "label_encode = preprocessing.LabelEncoder()  # 建立模型\n",
    "y_train = label_encode.fit_transform(y_train)\n",
    "y_dev = label_encode.transform(y_dev)\n",
    "y_test = label_encode.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sents_handler import generate_char_samples\n",
    "import numpy as np\n",
    "\n",
    "train_gs = generate_char_samples(np.array(text_train), np.array(y_train), vocab_file, 20, True)\n",
    "#sent_vecs, sent_labels, lengths = train_gs.generate(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vecs, sent_labels, lengths = train_gs.generate(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 22, 50)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the contextualized representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/vocab793471_embed100_filters64_batch32_layers3_block3_fdim5/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess_lm = tf.Session()\n",
    "saver.restore(sess_lm, conf.ckpt_file)\n",
    "def sent2vec(inputs, sess):\n",
    "    '''Get word representations'''\n",
    "    #Get the contextualized representation\n",
    "    #train_gs = generate_samples(np.array(text_train), np.array(y_train), word_to_idx, 20, False)\n",
    "    #sent_vecs, sent_labels, lengths = train_gs.generate(32)\n",
    "    out_layer = sess.run(model.out_layer, feed_dict={model.X:inputs})\n",
    "    return out_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "def sent_split(sent):\n",
    "    words = []\n",
    "    sent = nlp(sent.strip())\n",
    "    for w in sent:\n",
    "        words.append(w.text.lower())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/sswe-u.txt'\n",
    "word_emb = {}\n",
    "with open(file) as fi:\n",
    "    for line in fi:\n",
    "        items = line.split()\n",
    "        word_emb[items[0]] = np.array(items[1:], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(word, vocab_embed):\n",
    "    try:\n",
    "        vec = vocab_embed[word]\n",
    "    except:\n",
    "        vec = word_emb['<unk>']\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_len = 30\n",
    "hidden_size = 50\n",
    "class generate_samples:\n",
    "    '''\n",
    "    Generate samples for training data or testing data\n",
    "    '''\n",
    "    def __init__(self, data, labels, is_training=True):\n",
    "        '''\n",
    "        Args:\n",
    "        data: numpy\n",
    "        labels: numpy\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.is_training = is_training\n",
    "        self.index = 0\n",
    "        \n",
    "    def generate_samples(self, sents, labels, batch_size=64):\n",
    "        '''\n",
    "        Select a batch_size of sentences\n",
    "        Transform each sentence into a sequence of embeddings\n",
    "        '''\n",
    "        indice = np.random.choice(len(sents), batch_size)\n",
    "        sents = sents[indice]\n",
    "        labels = labels[indice]\n",
    "        sent_vecs, sent_lens = self.create_sent_emb(sents)\n",
    "        return sent_vecs, labels, sent_lens\n",
    "    \n",
    "    def create_sent_emb(self, sents):\n",
    "        '''\n",
    "        Create sequences of word embeddings for sentences\n",
    "        '''\n",
    "        sent_vecs = []#A matrix represents a sentence\n",
    "        sent_lens = []#length of each sentence\n",
    "        for sent in sents:\n",
    "            #print(type(sent))\n",
    "            words = sent_split(str(sent))\n",
    "            sent_lens.append(len(words))\n",
    "            sent_vec = []\n",
    "            for word in words:\n",
    "                vec = word2vec(word, word_emb)\n",
    "                sent_vec.append(vec)\n",
    "            #Cut long sentence\n",
    "            if len(words) > max_word_len:\n",
    "                sent_vec = sent_vec[:max_word_len]\n",
    "            #Pad short sentence\n",
    "            if len(words) < max_word_len:\n",
    "                for _ in np.arange(max_word_len - len(words)):\n",
    "                    vec = np.zeros(hidden_size)\n",
    "                    sent_vec.append(vec)\n",
    "            sent_vec = np.stack(sent_vec)\n",
    "            sent_vecs.append(sent_vec)\n",
    "        sent_vecs = np.stack(sent_vecs)\n",
    "        return sent_vecs, sent_lens\n",
    "        \n",
    "    def generate(self, batch_size=64):\n",
    "        if self.is_training:\n",
    "            sent_vecs, sent_labels, lengths = self.generate_samples(self.data, \n",
    "                                                               self.labels,\n",
    "                                                              batch_size)\n",
    "        else:\n",
    "            start = self.index\n",
    "            end = start + batch_size\n",
    "            if end > len(self.data):\n",
    "                print('Out of sample size')\n",
    "                self.index = 0\n",
    "            sents = self.data[start:end]\n",
    "            sent_labels = self.labels[start:end]\n",
    "            sent_vecs, lengths = self.create_sent_emb(sents)\n",
    "            self.index = end\n",
    "        return sent_vecs, sent_labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gs2 = generate_samples(np.array(text_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vecs, sent_labels, lengths = train_gs2.generate(32)\n",
    "#out_layer = sent2vec(sent_vecs, sess_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 22, 50)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_len = sent_vecs.shape[1]\n",
    "embed_size = 100\n",
    "from sentiment_analysis import CNN_Model_Pretrained_Emb\n",
    "class trainConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = embed_size\n",
    "    hidden_size = 250\n",
    "    batch_size = 32\n",
    "    layer_size = 2\n",
    "    \n",
    "class testConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = embed_size\n",
    "    hidden_size = 250\n",
    "    batch_size = 32\n",
    "    layer_size = 2\n",
    "    \n",
    "class singleConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = embed_size\n",
    "    hidden_size = 250#hidden size for hidden state of rnn\n",
    "    batch_size = 1\n",
    "    layer_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "graph_cnn = tf.Graph()\n",
    "#Create models for training and testing data\n",
    "with graph_cnn.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.02, 0.02)\n",
    "    with tf.name_scope('train'):\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            train_model = CNN_Model_Pretrained_Emb(trainConfig)\n",
    "            saver_sent=tf.train.Saver()\n",
    "    with tf.name_scope('test'):\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            test_model = CNN_Model_Pretrained_Emb(testConfig, False)\n",
    "            single_model = CNN_Model_Pretrained_Emb(singleConfig, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunk_num  = int(len(text_train)/trainConfig.batch_size)\n",
    "test_chunk_num = int(len(text_test)/testConfig.batch_size)\n",
    "remain_num = len(text_test) - testConfig.batch_size*test_chunk_num\n",
    "remain_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6932\n",
      "Loss: 0.7398\n",
      "Epoch 0 time:30.33\n",
      "Loss: 0.7677\n",
      "Loss: 0.5518\n",
      "Epoch 1 time:60.27\n",
      "Loss: 0.6084\n",
      "Loss: 0.6271\n",
      "Epoch 2 time:90.26\n",
      "Loss: 0.6064\n",
      "Loss: 0.5883\n",
      "Epoch 3 time:120.03\n",
      "Loss: 0.6063\n",
      "Loss: 0.5117\n",
      "Epoch 4 time:149.83\n"
     ]
    }
   ],
   "source": [
    "import time, os\n",
    "epochs = 5\n",
    "#train_chunk_num = 10\n",
    "file = \"ckpt_cnn_pretrained_emb/cnn.ckpt\"\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    #Initialize parameters\n",
    "    init = tf.global_variables_initializer()\n",
    "    if not os.path.exists(\"ckpt_cnn_pretrained_emb\"):\n",
    "        os.mkdir('ckpt_cnn_pretrained_emb')\n",
    "    if os.path.exists(\"ckpt_cnn_pretrained_emb/cnn.ckpt.index\"):\n",
    "        saver_sent.restore(sess, file)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    start_time = time.time()\n",
    "    for m in range(epochs):\n",
    "        for i in range(train_chunk_num):\n",
    "            #sess.run(tf.assign(learning_rate, 0.002*((0.98)**m)))\n",
    "            x, y, lengths = train_gs.generate(trainConfig.batch_size)\n",
    "            x = sent2vec(x, sess_lm)\n",
    "            feed_dict = {train_model.x:x, train_model.y:y, train_model.lengths:lengths}\n",
    "            l, _ = sess.run([train_model.cost, train_model.optimize], feed_dict=feed_dict)\n",
    "            if i%100 == 0:\n",
    "                print('Loss:', round(l, 4))\n",
    "        end_time = time.time()\n",
    "        print('Epoch', m, 'time:{:.2f}'.format(end_time - start_time))\n",
    "        if m%2 == 0:\n",
    "            saver_sent.save(sess,'ckpt_cnn_pretrained_emb/cnn.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "INFO:tensorflow:Restoring parameters from ckpt_cnn_pretrained_emb/cnn.ckpt\n",
      "Parameters restored\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 20, 100) for Tensor 'test/Model/Placeholder_3:0', which has shape '(1, 22, 100)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-c448d6b79de9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         feed_dict = {single_model.x:x, single_model.y:y, \n\u001b[1;32m     24\u001b[0m                      single_model.lengths:lengths}\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1074\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1076\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1077\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (1, 20, 100) for Tensor 'test/Model/Placeholder_3:0', which has shape '(1, 22, 100)'"
     ]
    }
   ],
   "source": [
    "#Calculate Testing Accuracy\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    print('Testing...')\n",
    "    count = 0\n",
    "    #saver = tf.train.import_meta_graph('ckpt_cnn/cnn.ckpt.meta')\n",
    "    saver_sent.restore(sess,tf.train.latest_checkpoint('ckpt_cnn_pretrained_emb/'))\n",
    "    print('Parameters restored')\n",
    "    start_time = time.time()\n",
    "    test_gs2 = generate_samples(np.array(text_test), np.array(y_test), False)\n",
    "    test_gs = generate_char_samples(np.array(text_test), np.array(y_test),vocab_file, 20, False)\n",
    "    for _ in range(test_chunk_num):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_gs.generate(testConfig.batch_size)\n",
    "        x = sent2vec(x, sess_lm)\n",
    "        feed_dict = {test_model.x:x, test_model.y:y, test_model.lengths:lengths}\n",
    "        n = sess.run(test_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    for i in range(remain_num):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_gs.generate(1)\n",
    "        #print(i, len(x))\n",
    "        x = sent2vec(x, sess_lm)\n",
    "        feed_dict = {single_model.x:x, single_model.y:y, \n",
    "                     single_model.lengths:lengths}\n",
    "        n = sess.run(single_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    end_time = time.time()\n",
    "    print('Testing Time:{:.2f}'.format(end_time - start_time))\n",
    "    print(count*1.0/len(text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7275894836870447\n"
     ]
    }
   ],
   "source": [
    "print(count*1.0/len(text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_lm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2318.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count+21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3157"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7342413683877098"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2318/3157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
