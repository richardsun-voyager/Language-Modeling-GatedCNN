{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep contextualized word representation has drawn wide attention because of state-of-the-art performances in downstream tasks. Contextualized embeddings can capture not only word-level information but also multi-sense information, thus improving the results in sentiment analysis, SQuad and etc. However, the language adopted in the [Elmo](https://allennlp.org/elmo) model were biLSTMs which contained a huge number of parameters, it was less likely for small labs to train and run such experiments.\n",
    "\n",
    "\n",
    "In this project, we intend to make use of CNN language model in learning efficient word representations for sentiment analysis. We train a language model based on [Gated CNN architecture](https://arxiv.org/abs/1612.08083) proposed by Yann Daulphin, then do sentiment analysis with embeddings generated by the language model.\n",
    "\n",
    "The language model training dataset is 1-billion-word-language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from bilm.training import load_options_latest_checkpoint, load_vocab\n",
    "from bilm.data import Batcher, BidirectionalLMDataset\n",
    "#from data_utils import data_helper\n",
    "from conf_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the configuration and prepare data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/wiki.test.tokens') as f:\n",
    "#     lines = f.readlines()\n",
    "#     lines = [line for line in lines if len(line)>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range28):\n",
    "#     start = i * 100\n",
    "#     end = (i+1) * 100\n",
    "#     texts = lines[start:end]\n",
    "#     with open('data/wikitext-103-test/text'+str(i), 'w+') as f:\n",
    "#         for line in texts:\n",
    "#             f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(50):\n",
    "#     start = i * 22000\n",
    "#     end = (i+1) * 22000\n",
    "#     texts = lines[start:end]\n",
    "#     with open('data/wikitext-103/text'+str(i), 'w+') as f:\n",
    "#         for line in texts:\n",
    "#             f.write(line+'\\n')\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/wiki-vocab.txt', 'w+') as f:\n",
    "#     f.write('</S>\\n')\n",
    "#     f.write('<S>\\n')\n",
    "#     f.write('<UNK>\\n')\n",
    "#     for line in lines:\n",
    "#         word = line.split()[0]\n",
    "#         f.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('data/vocab-2016-09-10.txt') as f:\n",
    "    #lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the words\n",
    "vocab_file = 'data/vocab-2016-09-10.txt'\n",
    "#vocab_file = 'data/wiki-vocab.txt'\n",
    "vocab = load_vocab(vocab_file, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_size = vocab.size\n",
    "    embedding_size = 128\n",
    "    filter_size = 64\n",
    "    num_layers = 3\n",
    "    block_size = 3\n",
    "    filter_h = 5\n",
    "    context_size = 50\n",
    "    text_size = context_size\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    num_sampled = 64\n",
    "    learning_rate = 3\n",
    "    momentum = 0.99\n",
    "    grad_clip = 0.1\n",
    "    num_batches = 0\n",
    "    ckpt_path = 'ckpt_char_gated_cnn'\n",
    "    summary_path = 'logs'\n",
    "    #data_dir = \"data/texts/reviews/movie_reviews\"\n",
    "    data_dir = \"data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize configuration files\n",
    "conf = prepare_conf(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prefix = 'data/1-billion-word-language-modeling-ben\\\n",
    "chmark-r13output/training-monolingual.tokenized.shuffled/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_prefix = 'data/wikitext-103/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 shards at data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00034-of-00100\n",
      "Loaded 305408 sentences.\n",
      "Finished loading\n",
      "Found 50 shards at data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*\n",
      "Loading data from: data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00059-of-00100\n",
      "Loaded 306839 sentences.\n",
      "Finished loading\n"
     ]
    }
   ],
   "source": [
    "data = BidirectionalLMDataset(train_prefix, vocab, test=False,\n",
    "                                      shuffle_on_load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = data.iter_batches(conf.batch_size * 1, conf.text_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token_ids', 'tokens_characters', 'next_token_id', 'token_ids_reverse', 'tokens_characters_reverse', 'next_token_id_reverse'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a CharCNN-based language model\n",
    "\n",
    "Note the inputs are transformed into chars of words, so as to make use of subword information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 277714850 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Model Training...\n"
     ]
    }
   ],
   "source": [
    "#Create a language model\n",
    "#Note we need to save the models for subsequent tasks\n",
    "from bi_char_cnn_lm_model import gated_char_cnn_model\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope('gated_cnn'):\n",
    "        model = gated_char_cnn_model(conf, is_bidirectional=True)\n",
    "        all_variables = tf.get_collection_ref(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        var_list=[v for v in all_variables if \"Adagrad\" not in v.name]\n",
    "    with tf.variable_scope('gated_cnn', reuse=True):\n",
    "        model_test = gated_char_cnn_model(conf, is_train=False, is_bidirectional=True)\n",
    "    saver = tf.train.Saver(var_list=var_list)\n",
    "    print(\"Started Model Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 loop0 30.012493\n"
     ]
    }
   ],
   "source": [
    "batch_idx = 0\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter(conf.summary_path, graph=sess.graph)\n",
    "\n",
    "    if os.path.exists(conf.ckpt_file+'.index'):\n",
    "        saver.restore(sess, conf.ckpt_file)\n",
    "        print(\"Model Restored\")\n",
    "\n",
    "    for i in np.arange(conf.epochs):\n",
    "        start = time.time()\n",
    "        for j in np.arange(10000):\n",
    "        #for j in np.arange(21):\n",
    "            x = next(data_gen)\n",
    "            inputs, labels = x['tokens_characters'], x['next_token_id']\n",
    "            inputs_reverse, labels_reverse = x['tokens_characters_reverse'], \\\n",
    "                                                    x['next_token_id_reverse']\n",
    "            labels = labels.reshape(-1, 1)\n",
    "            labels_reverse = labels_reverse.reshape(-1, 1)\n",
    "            feed_dict = {model.X:inputs, model.y:labels, \n",
    "                         model.X_reverse:inputs_reverse, model.y_reverse:labels_reverse}\n",
    "            _, l = sess.run([model.optimizer, model.loss], \n",
    "                            feed_dict=feed_dict)\n",
    "            if j%200 == 0:\n",
    "                print('epoch'+str(i), 'loop'+str(j), l)\n",
    "            if j%2000 == 1999:\n",
    "                perp = sess.run(model.perplexity, \n",
    "                                feed_dict=feed_dict)\n",
    "                print(\"Perplexity: %.2f\"%perp)\n",
    "                saver.save(sess, conf.ckpt_file)\n",
    "        end = time.time()\n",
    "        print(\"Epoch: %.2f, Time: %.2f,  Loss: %.2f\"%(i, end-start, l))\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            perp = sess.run(model.perplexity, feed_dict={model.X:inputs, model.y:labels})\n",
    "            print(\"Perplexity: %.2f\"%perp)\n",
    "            saver.save(sess, conf.ckpt_file)\n",
    "\n",
    "        #summaries = sess.run(model.merged_summary_op, feed_dict={model.X:inputs, model.y:labels})\n",
    "        #summary_writer.add_summary(summaries, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testConfig:\n",
    "    vocab_size = vocab.size\n",
    "    embedding_size = 128\n",
    "    filter_size = 64\n",
    "    num_layers = 3\n",
    "    block_size = 3\n",
    "    filter_h = 5\n",
    "    context_size = 50\n",
    "    text_size = context_size\n",
    "    batch_size = 32\n",
    "    epochs = 8\n",
    "    num_sampled = 64\n",
    "    learning_rate = 1\n",
    "    momentum = 0.995\n",
    "    grad_clip = 0.1\n",
    "    num_batches = 0\n",
    "    ckpt_path = 'ckpt_nlp_block'\n",
    "    summary_path = 'logs'\n",
    "    #data_dir = \"data/texts/reviews/movie_reviews\"\n",
    "    data_dir = \"data/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 shards at data/wikitext-103-test/*\n",
      "Loading data from: data/wikitext-103-test/text17\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Found 28 shards at data/wikitext-103-test/*\n",
      "Loading data from: data/wikitext-103-test/text17\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n"
     ]
    }
   ],
   "source": [
    "test_prefix = 'data/wikitext-103-test/*'\n",
    "data_test = BidirectionalLMDataset(test_prefix, vocab, test=True,\n",
    "                                      shuffle_on_load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_test = data_test.iter_batches(conf.batch_size * 1, conf.text_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt_char_gated_cnn/vocab267743_embed128_filters64_batch64_layers3_block3_fdim5/model.ckpt\n",
      "Model Restored\n",
      "Tesing Loss: 4.6965504\n",
      "Loading data from: data/wikitext-103-test/text13\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text13\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text8\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text8\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text14\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text14\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text9\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text9\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text1\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text1\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text23\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text23\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text24\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text24\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text3\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text3\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text2\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text2\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text10\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text10\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text27\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text27\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text5\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text5\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text20\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text20\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text22\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text22\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text26\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text26\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text7\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text7\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text19\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text19\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text6\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text6\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text25\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text25\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text21\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text21\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text11\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text11\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text18\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text18\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text15\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text15\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text4\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text4\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text0\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text0\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text16\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text16\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text12\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n",
      "Loading data from: data/wikitext-103-test/text12\n",
      "Loaded 200 sentences.\n",
      "Finished loading\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f2084389accc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#for j in np.arange(21):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens_characters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'next_token_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m#inputs_r, labels_r = x['token_ids_reverse'], x['next_token_id_reverse']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###Testting\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    if os.path.exists(conf.ckpt_file+'.index'):\n",
    "        saver.restore(sess, conf.ckpt_file)\n",
    "        print(\"Model Restored\")\n",
    "    losses = []\n",
    "    for j in np.arange(10000):\n",
    "        #for j in np.arange(21):\n",
    "            x = next(data_gen_test)\n",
    "            inputs, labels = x['tokens_characters'], x['next_token_id']\n",
    "            #inputs_r, labels_r = x['token_ids_reverse'], x['next_token_id_reverse']\n",
    "            feed_dict = {model_test.X:inputs, model_test.y:labels}\n",
    "            l = sess.run(model_test.loss, feed_dict={model_test.X:inputs, model_test.y:labels})\n",
    "            if j%100 == 0:\n",
    "                print('Tesing Loss:', l)\n",
    "            losses.append(l)\n",
    "    print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "In this part, we need to use other datasets for sentiment analysis, like the one of SemEval2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = 'data/semeval/downloaded.tsv'\n",
    "file_dev = 'data/semeval/dev_downloaded.tsv'\n",
    "file_test = 'data/semeval/test.txt'\n",
    "with open(file_train) as f:\n",
    "    tweets_train = f.readlines()\n",
    "with open(file_dev) as f:\n",
    "    tweets_dev = f.readlines()\n",
    "with open(file_test) as f:\n",
    "    tweets_test = f.readlines()\n",
    "    \n",
    "\n",
    "\n",
    "#Filter empty tweets\n",
    "def is_available(text):\n",
    "    if 'Not Available' in text:\n",
    "        return False\n",
    "    if '\\t\"objective' in text:\n",
    "        return False\n",
    "    if '\\t\"neutral' in text:\n",
    "        return False\n",
    "    if '\\tobjective' in text:\n",
    "        return False\n",
    "    if '\\tneutral' in text:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = list(filter(is_available, tweets_train))\n",
    "tweets_dev = list(filter(is_available, tweets_dev))\n",
    "tweets_test = list(filter(is_available, tweets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = [item.split('\\t') for item in tweets_train]\n",
    "tweets_dev = [item.split('\\t') for item in tweets_dev]\n",
    "tweets_test = [item.split('\\t') for item in tweets_test]\n",
    "_, _, y_train, text_train = list(zip(*tweets_train))\n",
    "_, _, y_dev, text_dev = list(zip(*tweets_dev))\n",
    "_, _, y_test, text_test = list(zip(*tweets_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, y_train = list(text_train), list(y_train)\n",
    "text_dev, y_dev = list(text_dev), list(y_dev)\n",
    "text_test, y_test = list(text_test), list(y_test)\n",
    "y_test = ['\"' + item + '\"' for item in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the labels to numbers\n",
    "from sklearn import preprocessing\n",
    "label_encode = preprocessing.LabelEncoder()  # 建立模型\n",
    "y_train = label_encode.fit_transform(y_train)\n",
    "y_dev = label_encode.transform(y_dev)\n",
    "y_test = label_encode.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sents_handler import generate_char_samples\n",
    "import numpy as np\n",
    "\n",
    "train_gs = generate_char_samples(np.array(text_train), np.array(y_train), vocab_file, 50, True)\n",
    "#sent_vecs, sent_labels, lengths = train_gs.generate(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the contextualized representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt_char_gated_cnn/vocab267743_embed128_filters64_batch64_layers3_block3_fdim5/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess_lm = tf.Session(graph=graph)\n",
    "saver.restore(sess_lm, conf.ckpt_file)\n",
    "def sent2vec(inputs, sess):\n",
    "    '''Get word representations'''\n",
    "    #Get the contextualized representation\n",
    "    #train_gs = generate_samples(np.array(text_train), np.array(y_train), word_to_idx, 20, False)\n",
    "    #sent_vecs, sent_labels, lengths = train_gs.generate(32)\n",
    "    out_layer = sess.run(model.hidden_layers, feed_dict={model.X:inputs})\n",
    "    return out_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vecs, sent_labels, lengths = train_gs.generate(64)\n",
    "def sent_emb_padding(sent_vecs):\n",
    "    shape = sent_vecs.shape\n",
    "    sent_char_matrix = np.zeros([64, 50, 50])\n",
    "    if shape[1] < 50:\n",
    "        sent_char_matrix[:, :shape[1], :] = sent_vecs[:, :, :]\n",
    "    else:\n",
    "        sent_char_matrix = sent_vecs\n",
    "    return sent_char_matrix    \n",
    "sent_vecs = sent_emb_padding(sent_vecs)\n",
    "out_layer = sent2vec(sent_vecs, sess_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 50, 50)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_len = out_layer.shape[1]\n",
    "from sentiment_analysis import CNN_Model_Pretrained_Emb\n",
    "class trainConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = 350\n",
    "    hidden_size = 250\n",
    "    batch_size = 64\n",
    "    layer_size = 2\n",
    "    \n",
    "class testConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = 350\n",
    "    hidden_size = 250\n",
    "    batch_size = 64\n",
    "    layer_size = 2\n",
    "    \n",
    "class singleConfig:\n",
    "    max_doc_len = max_word_len\n",
    "    label_size = 2\n",
    "    embed_size = 350\n",
    "    hidden_size = 250#hidden size for hidden state of rnn\n",
    "    batch_size = 1\n",
    "    layer_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n",
      "Model Initialized!\n",
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "graph_cnn = tf.Graph()\n",
    "#Create models for training and testing data\n",
    "with graph_cnn.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.01, 0.01)\n",
    "    with tf.name_scope('train'):\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            train_model = CNN_Model_Pretrained_Emb(trainConfig)\n",
    "            saver_sent=tf.train.Saver()\n",
    "    with tf.name_scope('test'):\n",
    "        #Set different models for different buckets\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            test_model = CNN_Model_Pretrained_Emb(testConfig, False)\n",
    "            single_model = CNN_Model_Pretrained_Emb(singleConfig, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunk_num  = int(len(text_train)/trainConfig.batch_size)\n",
    "test_chunk_num = int(len(text_test)/testConfig.batch_size)\n",
    "remain_num = len(text_test) - testConfig.batch_size*test_chunk_num\n",
    "remain_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt_cnn_pretrained_emb/cnn.ckpt\n",
      "Loss: 0.6661\n",
      "Epoch 0 time:21.20\n",
      "Loss: 0.5504\n",
      "Epoch 1 time:42.21\n",
      "Loss: 0.5348\n"
     ]
    }
   ],
   "source": [
    "import time, os\n",
    "epochs = 5\n",
    "#train_chunk_num = 10\n",
    "file = \"ckpt_cnn_pretrained_emb/cnn.ckpt\"\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    #Initialize parameters\n",
    "    init = tf.global_variables_initializer()\n",
    "    if not os.path.exists(\"ckpt_cnn_pretrained_emb\"):\n",
    "        os.mkdir('ckpt_cnn_pretrained_emb')\n",
    "    if os.path.exists(\"ckpt_cnn_pretrained_emb/cnn.ckpt.index\"):\n",
    "        saver_sent.restore(sess, file)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    start_time = time.time()\n",
    "    for m in range(epochs):\n",
    "        for i in range(train_chunk_num):\n",
    "            #sess.run(tf.assign(learning_rate, 0.002*((0.98)**m)))\n",
    "            x, y, lengths = train_gs.generate(trainConfig.batch_size)\n",
    "            x1 = sent_emb_padding(x)\n",
    "            x2 = sent2vec(x1, sess_lm)\n",
    "            feed_dict = {train_model.x:x2, train_model.y:y, train_model.lengths:lengths}\n",
    "            l, _ = sess.run([train_model.cost, train_model.optimize], feed_dict=feed_dict)\n",
    "            if i%100 == 0:\n",
    "                print('Loss:', round(l, 4))\n",
    "        end_time = time.time()\n",
    "        print('Epoch', m, 'time:{:.2f}'.format(end_time - start_time))\n",
    "        if m%2 == 0:\n",
    "            saver_sent.save(sess,'ckpt_cnn_pretrained_emb/cnn.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "INFO:tensorflow:Restoring parameters from ckpt_cnn_pretrained_emb/cnn.ckpt\n",
      "Parameters restored\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64, 50, 350) for Tensor 'test/Model/Placeholder_3:0', which has shape '(1, 50, 350)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-dd0f66eb345f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         feed_dict = {single_model.x:x, single_model.y:y, \n\u001b[1;32m     25\u001b[0m                      single_model.lengths:lengths}\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1074\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1076\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1077\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (64, 50, 350) for Tensor 'test/Model/Placeholder_3:0', which has shape '(1, 50, 350)'"
     ]
    }
   ],
   "source": [
    "#Calculate Testing Accuracy\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    print('Testing...')\n",
    "    count = 0\n",
    "    #saver = tf.train.import_meta_graph('ckpt_cnn/cnn.ckpt.meta')\n",
    "    saver_sent.restore(sess,tf.train.latest_checkpoint('ckpt_cnn_pretrained_emb/'))\n",
    "    print('Parameters restored')\n",
    "    start_time = time.time()\n",
    "    test_gs = generate_char_samples(np.array(text_test), np.array(y_test),vocab_file, 20, False)\n",
    "    for _ in range(test_chunk_num):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_gs.generate(testConfig.batch_size)\n",
    "        x = sent_emb_padding(x)\n",
    "        x = sent2vec(x, sess_lm)\n",
    "        feed_dict = {test_model.x:x, test_model.y:y, test_model.lengths:lengths}\n",
    "        n = sess.run(test_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    for i in range(remain_num):\n",
    "        #Traverse each data\n",
    "        x, y, lengths = test_gs.generate(1)\n",
    "        x = sent_emb_padding(x)\n",
    "        #print(i, len(x))\n",
    "        x = sent2vec(x, sess_lm)\n",
    "        feed_dict = {single_model.x:x, single_model.y:y, \n",
    "                     single_model.lengths:lengths}\n",
    "        n = sess.run(single_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(n)\n",
    "    end_time = time.time()\n",
    "    print('Testing Time:{:.2f}'.format(end_time - start_time))\n",
    "    print(count*1.0/len(text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7260057016154577\n"
     ]
    }
   ],
   "source": [
    "print(count*1.0/len(text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_lm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
